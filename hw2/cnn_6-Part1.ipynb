{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyh0knhqJx8u"
   },
   "source": [
    "понятное [видео](https://www.youtube.com/watch?v=bNb2fEVKeEo) со стенфордского курса, из [материалов](https://cs231n.github.io/convolutional-networks) по которому взяты иллюстрации.\n",
    "\n",
    "Датасет из [курса](https://github.com/DanAnastasyev/DeepNLP-Course/blob/master/Week%2004/Week_04_Convolutional_Neural_Networks.ipynb) Даниила Анастасьева.\n",
    "\n",
    "Использовались [материалы](https://github.com/mannefedov/hse_ml_m1/blob/master/7_cnn/cnn.ipynb) из курса Михаила Нефедова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdbHKxg6J8Q3"
   },
   "source": [
    "# Сверточный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G20z18IaJ6Y5"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Желтое - фильтр (= filter,kernel).\n",
    "\n",
    "Зеленое - входные данные, например, изображение.\n",
    "\n",
    "Розовое - карта активации (activation map).\n",
    "\n",
    "Каждый элемент в розовой матрице - результат поэлементного умножения фильтра на числа из области на входных данных.\n",
    "Обучаемые параметры - элементы фильтра.\n",
    "\n",
    "![Conv](https://image.ibb.co/e6t8ZK/Convolution.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXNixkx2KHsm"
   },
   "source": [
    "Чтобы не терять размер матрицы используется паддинг.\n",
    "\n",
    "![padding](https://3deep.ru/wp-content/uploads/2020/01/keras_conv2d_padding.gif)\n",
    "\n",
    "from https://3deep.ru/machinelearning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp44YeUKKSem"
   },
   "source": [
    "# Pooling слой  (не обучается)\n",
    "\n",
    "![Pool](https://cs231n.github.io/assets/cnn/pool.jpeg)\n",
    "![maxpool](https://cs231n.github.io/assets/cnn/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VqCU9o5KUmr"
   },
   "source": [
    "# Свертки для текстов устроены немного по-другому. В них на одну размерность меньше.\n",
    "\n",
    "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png)\n",
    "\n",
    "From [Character-Aware Neural Language Models](https://arxiv.org/pdf/1508.06615.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXx-iELMKUjt"
   },
   "source": [
    "# CNN для обработки текстов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_LxOOJUyn-E",
    "outputId": "6ad4b0f7-9eaf-4e43-e4db-103ce1603684"
   },
   "outputs": [],
   "source": [
    "#!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NovB599mOf9T",
    "outputId": "9f83601f-e2c1-4d76-e7da-294805cb15eb"
   },
   "outputs": [],
   "source": [
    "#!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CgiT9Xow1sd5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torchmetrics import F1\n",
    "from torchmetrics.functional import f1, recall\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfxNXovr1sd6"
   },
   "source": [
    "### Слова\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZtLedfF00Ih"
   },
   "source": [
    "### Скачивание и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEBb-YaTR1os",
    "outputId": "5d4e7170-3a62-4000-8b04-64e79c0179bf"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "#wget.download(\"https://drive.google.com/uc?export=download&id=1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {1:1, -1:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "17_AP7LhLPnv"
   },
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive.csv', encoding='utf-8', sep=';', usecols=[3,4], names=['text', 'type'])\n",
    "negative = pd.read_csv('negative.csv', encoding='utf-8', sep=';', usecols=[3,4], names=['text', 'type'])\n",
    "data = pd.concat([positive, negative])\n",
    "data['type'] = [type_dict[i] for i in data['type']]\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "_JYJtXCaERbb"
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, shuffle=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfA7yoOFA4cm",
    "outputId": "ed113ddd-a4c9-4019-cef1-a60898ecd691"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075595566593083"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.type == 1].shape[0] / train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "45ebLSllFFAW",
    "outputId": "dc094ba1-a780-4e5c-e784-3101463299b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Карманная Катяя)\\nТы знаешь что делать...\\nИде...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Скоро это чудо будет моей:*** http://t.co/iRWH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ronniemustdie ну из своего города я только па...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AsikLR не печалься:3 держи Томушку;)) http://...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@tutby так я ж не видел факт публикования ваши...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Elya_Mangusheva @VorontsovaAnya @Igor995  она...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@SatanLovesFangs а люди в ВК пишут, что это ре...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@Vika_Tweet круто повезло, что она выиграла фо...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @Tatyana_98_: Эта картинка Ларри раздирает ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@HORAN_NlALL И тут я запутался авхазхзвахазхзв...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @Pdv18: Шок(((((,Илья,светлый добрый,искрен...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@demkristo Учёба для лохов - как не быть лохом...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@GREENloshadka уиии, круто)\\nбеларусь, минск\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Да ты успокойся, я же сказала до субботы подум...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Меня на фб из избранных авторов кто-то убрал :...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>день был невероятен,но было одно очень плохое ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@Ninnel добрый вечер! Это вы губернатору нашем...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Я просто победитель по жизни!!дырка в ноге кру...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@_iambeechelon_ @ierowaybja я приличная девочк...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ибо в Серые Стражи достойны вступать только из...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>сейчас я знаю дохуя пацанов с мгн, потом я уед...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Мама сказала пока дневник не заполню , спать н...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RT @Aline_Bogun: @YankaMashenko @broken_260599...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Решил снести джейлбрейк. Восстановил iPad и…уд...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>всегда хотела заниматься благотворительностью ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Как же я устала... как же это сложно...это всё...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Хочу поздравить с днем рождения чудесную @mari...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>за 7 лет выступлений, именно такое со мной пер...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ИТАК,ЛЕНТА,Я СЕГОДНЯ САНТА,КОТОРЫЙ ИСПОЛНЯЕТ Ж...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@walter_tanyaZ просто сидишь? а я историю дела...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Не имей сто друзей, а имей их подруг!) http://...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Странно. В моем Гугле сраный трактор до сих по...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RT @natunegy: Ты такая классная, когда выпьешь...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>да нуу,наоборот пусть и оскорбления будут тоже...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>@altynai_dju по ходу завтра я как последнее чм...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RT @RadoSt_Mi: @dikadog_9 классное.. я тож так...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Дорогой!..Ты - привы4ка...только жаль, что вре...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>@ViksBatman ты всегда спишь  с:\\n\\nполдня я сп...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6  часов, самое время праздновать новый год(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RT @GeroiGroup: На съемочной площадке встретил...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>я буду скучать по этим людям, жалко тут не все...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>#фолловинг блин, когда же будет полный перевод...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>@Katya_Jed эээээй, ахахахахахахах:D\\nхороший д...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>@Nastya_infinity кстати, я очень расстроен тем...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Почему-то людям стало свойственно превышать св...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>http://t.co/vzlmXhD5ww а я смотрю, чего то не ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Хочется хотя бы на минутку взглянуть на тебя, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>@Katy_mrrr тебе сфоткать как он трахает мишку ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Ой, он уже пожарился.\\nНе судьба (((9(9(((9((9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>То, что нг, через 3 дня, вообще не ощущается :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  type\n",
       "0   Карманная Катяя)\\nТы знаешь что делать...\\nИде...     1\n",
       "1   Скоро это чудо будет моей:*** http://t.co/iRWH...     1\n",
       "2   @ronniemustdie ну из своего города я только па...     0\n",
       "3   @AsikLR не печалься:3 держи Томушку;)) http://...     1\n",
       "4   @tutby так я ж не видел факт публикования ваши...     1\n",
       "5   @Elya_Mangusheva @VorontsovaAnya @Igor995  она...     1\n",
       "6   @SatanLovesFangs а люди в ВК пишут, что это ре...     0\n",
       "7   @Vika_Tweet круто повезло, что она выиграла фо...     1\n",
       "8   RT @Tatyana_98_: Эта картинка Ларри раздирает ...     0\n",
       "9   @HORAN_NlALL И тут я запутался авхазхзвахазхзв...     1\n",
       "10  RT @Pdv18: Шок(((((,Илья,светлый добрый,искрен...     0\n",
       "11  @demkristo Учёба для лохов - как не быть лохом...     1\n",
       "12  @GREENloshadka уиии, круто)\\nбеларусь, минск\\n...     1\n",
       "13  Да ты успокойся, я же сказала до субботы подум...     0\n",
       "14  Меня на фб из избранных авторов кто-то убрал :...     0\n",
       "15  день был невероятен,но было одно очень плохое ...     0\n",
       "16  @Ninnel добрый вечер! Это вы губернатору нашем...     1\n",
       "17  Я просто победитель по жизни!!дырка в ноге кру...     0\n",
       "18  @_iambeechelon_ @ierowaybja я приличная девочк...     0\n",
       "19  ибо в Серые Стражи достойны вступать только из...     0\n",
       "20  сейчас я знаю дохуя пацанов с мгн, потом я уед...     0\n",
       "21  Мама сказала пока дневник не заполню , спать н...     0\n",
       "22  RT @Aline_Bogun: @YankaMashenko @broken_260599...     0\n",
       "23  Решил снести джейлбрейк. Восстановил iPad и…уд...     0\n",
       "24  всегда хотела заниматься благотворительностью ...     0\n",
       "25  Как же я устала... как же это сложно...это всё...     0\n",
       "26  Хочу поздравить с днем рождения чудесную @mari...     1\n",
       "27  за 7 лет выступлений, именно такое со мной пер...     0\n",
       "28  ИТАК,ЛЕНТА,Я СЕГОДНЯ САНТА,КОТОРЫЙ ИСПОЛНЯЕТ Ж...     1\n",
       "29  @walter_tanyaZ просто сидишь? а я историю дела...     0\n",
       "30  Не имей сто друзей, а имей их подруг!) http://...     1\n",
       "31  Странно. В моем Гугле сраный трактор до сих по...     1\n",
       "32  RT @natunegy: Ты такая классная, когда выпьешь...     1\n",
       "33  да нуу,наоборот пусть и оскорбления будут тоже...     1\n",
       "34  @altynai_dju по ходу завтра я как последнее чм...     1\n",
       "35  RT @RadoSt_Mi: @dikadog_9 классное.. я тож так...     0\n",
       "36  Дорогой!..Ты - привы4ка...только жаль, что вре...     0\n",
       "37  @ViksBatman ты всегда спишь  с:\\n\\nполдня я сп...     0\n",
       "38       6  часов, самое время праздновать новый год(     0\n",
       "39  RT @GeroiGroup: На съемочной площадке встретил...     1\n",
       "40  я буду скучать по этим людям, жалко тут не все...     0\n",
       "41  #фолловинг блин, когда же будет полный перевод...     0\n",
       "42  @Katya_Jed эээээй, ахахахахахахах:D\\nхороший д...     1\n",
       "43  @Nastya_infinity кстати, я очень расстроен тем...     0\n",
       "44  Почему-то людям стало свойственно превышать св...     1\n",
       "45  http://t.co/vzlmXhD5ww а я смотрю, чего то не ...     1\n",
       "46  Хочется хотя бы на минутку взглянуть на тебя, ...     0\n",
       "47  @Katy_mrrr тебе сфоткать как он трахает мишку ...     1\n",
       "48     Ой, он уже пожарился.\\nНе судьба (((9(9(((9((9     0\n",
       "49    То, что нг, через 3 дня, вообще не ощущается :(     0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну что ж... Где-то здесь должен возникнуть препроцессинг, и он возникнет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download ru_core_news_sm\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text, nlp):\n",
    "    #doc = nlp(text)\n",
    "    #postags_stop = ['ADP', 'AUX', 'CCONJ', 'DET', 'INTJ', 'PART', 'PRON', 'PUNCT', 'SCONJ']\n",
    "    lemms = []\n",
    "    doc = re.split('[ \\n\\t,.]', text)\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        if word:\n",
    "            lemms.append(word.strip('#,.*\\'\\\"@!?-^$:;').rstrip('():***').lower())\n",
    "        #lemms.append(word.lemma_.lower())\n",
    "    return ' '.join(lemms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greenloshadka уиии круто беларусь минск'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('@GREENloshadka уиии, круто)\\nбеларусь, минск\\n...\t', nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-q1JHzW1sd-"
   },
   "source": [
    "Теперь нам нужно попрепроцессить тексты. Или оставить пока так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему мы можем трогать только train_data? Потому что иначе может попасть что-нибудь из теста. Вероятно, будет не очень заментно, но исключать возможность не стоит, и на уровне слов она больше, чем на уровне символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8t4nDCR1sd-",
    "outputId": "d9acca2c-31d3-463d-a2d2-6bcdc127db49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 173952/173952 [00:02<00:00, 85666.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего уникальных слов: 290892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts_of_lemms = []\n",
    "vocab = Counter()\n",
    "for text in tqdm(train_data['text']):\n",
    "    lemma_text = preprocess(text, nlp)\n",
    "    vocab.update(lemma_text.split(' '))\n",
    "    texts_of_lemms = []\n",
    "print('всего уникальных слов:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "TFlVinuMObSL",
    "outputId": "fed8ab1c-c343-4b90-f432-6a27190e3cba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'только': 2709,\n",
       "         'прийти': 640,\n",
       "         'домой': 484,\n",
       "         ')': 42209,\n",
       "         '\\n': 13409,\n",
       "         'завтра': 1472,\n",
       "         'буду': 1272,\n",
       "         'спать': 1057,\n",
       "         'до': 1518,\n",
       "         'час': 836,\n",
       "         '10': 329,\n",
       "         'точно': 355,\n",
       "         'этот': 2858,\n",
       "         'фотография': 119,\n",
       "         'больше': 947,\n",
       "         'год': 1934,\n",
       "         'http://t.co/iwplejdinn': 1,\n",
       "         'rt': 10016,\n",
       "         '@das_patrick': 1,\n",
       "         ':': 14644,\n",
       "         'диана': 9,\n",
       "         'ждёт': 11,\n",
       "         'меня': 6368,\n",
       "         ',': 54123,\n",
       "         'пока': 652,\n",
       "         'я': 18954,\n",
       "         'у': 6444,\n",
       "         'стоматолог': 28,\n",
       "         ';': 1566,\n",
       "         '(': 44061,\n",
       "         '@blackbird_al': 1,\n",
       "         'ахахаха': 137,\n",
       "         'а': 10019,\n",
       "         'почему': 1127,\n",
       "         'училка': 38,\n",
       "         'так': 5530,\n",
       "         'написать': 611,\n",
       "         '?': 9252,\n",
       "         'она': 1365,\n",
       "         'в': 16325,\n",
       "         'тема': 197,\n",
       "         '@kgshel': 1,\n",
       "         'и': 17377,\n",
       "         'ещё': 789,\n",
       "         'впервые': 82,\n",
       "         'покраснеть': 2,\n",
       "         'щека': 27,\n",
       "         ':d': 4634,\n",
       "         'классный': 281,\n",
       "         'зрелище': 5,\n",
       "         'видимо': 179,\n",
       "         'не': 21561,\n",
       "         'зря': 124,\n",
       "         'тебе': 1689,\n",
       "         'разрешить': 30,\n",
       "         'намылить': 2,\n",
       "         'себя': 1443,\n",
       "         'поставить': 248,\n",
       "         'wow': 11,\n",
       "         'поиграть': 52,\n",
       "         'рф': 28,\n",
       "         'на': 11073,\n",
       "         '\"': 6638,\n",
       "         'пиратский': 4,\n",
       "         'бухта': 3,\n",
       "         '.': 17287,\n",
       "         'бухтой': 1,\n",
       "         'остаться': 475,\n",
       "         'недовольный': 10,\n",
       "         ':))': 449,\n",
       "         'мало': 301,\n",
       "         'народ': 126,\n",
       "         'впрочем': 22,\n",
       "         'потому': 509,\n",
       "         'что': 11335,\n",
       "         '3': 635,\n",
       "         'день': 2906,\n",
       "         'звонить': 146,\n",
       "         'сестра': 116,\n",
       "         'разговаривать': 90,\n",
       "         'мочь': 2562,\n",
       "         'работа': 708,\n",
       "         'еще': 2417,\n",
       "         'раз': 1118,\n",
       "         '2': 781,\n",
       "         'неделя': 581,\n",
       "         'видеться': 49,\n",
       "         ':(': 9097,\n",
       "         'быть': 2451,\n",
       "         'такой': 3241,\n",
       "         'человек': 1731,\n",
       "         'которым': 44,\n",
       "         'расскажешь': 9,\n",
       "         'то': 5825,\n",
       "         'по': 3643,\n",
       "         'секрет': 35,\n",
       "         'внутри': 48,\n",
       "         'них': 344,\n",
       "         'нажимается\"мне': 1,\n",
       "         'нравиться': 569,\n",
       "         'распездеть': 1,\n",
       "         'всем:-': 1,\n",
       "         'уууу': 12,\n",
       "         'сегодня': 2523,\n",
       "         'др': 175,\n",
       "         'зейна': 6,\n",
       "         'моей': 9,\n",
       "         'ленте': 1,\n",
       "         'все': 3464,\n",
       "         'уже': 3164,\n",
       "         'его': 1418,\n",
       "         'поздравляют': 1,\n",
       "         'проснулся': 2,\n",
       "         'капец': 202,\n",
       "         'школа': 885,\n",
       "         '-': 9687,\n",
       "         'за': 3519,\n",
       "         'любить': 1584,\n",
       "         'привет': 370,\n",
       "         'делать': 1254,\n",
       "         'курю': 12,\n",
       "         'сигары': 2,\n",
       "         'пить': 219,\n",
       "         'абсент': 3,\n",
       "         'управлять': 9,\n",
       "         'жизнями': 2,\n",
       "         'других': 66,\n",
       "         'опять': 701,\n",
       "         'ld,\"тархун': 1,\n",
       "         'sims': 4,\n",
       "         'ты': 4446,\n",
       "         'знать': 1952,\n",
       "         '@uselessmouth': 7,\n",
       "         'ни': 781,\n",
       "         'разу': 80,\n",
       "         'даже': 1545,\n",
       "         'туда': 180,\n",
       "         'заходить': 153,\n",
       "         'испугаться': 32,\n",
       "         'ли': 481,\n",
       "         ':)': 5437,\n",
       "         'http://t.co/rqh30ui5no': 1,\n",
       "         'ocean': 1,\n",
       "         'plaza': 4,\n",
       "         'преображаться': 1,\n",
       "         'дарить': 71,\n",
       "         'всем': 439,\n",
       "         'красивый': 335,\n",
       "         'новогодний': 326,\n",
       "         'настроение': 870,\n",
       "         '*': 1709,\n",
       "         '_': 2894,\n",
       "         'свято': 5,\n",
       "         'наближаеться': 3,\n",
       "         '!': 16577,\n",
       "         '@': 257,\n",
       "         '…': 1044,\n",
       "         'http://t.co/p9b7zj7yqe': 1,\n",
       "         '@lola_tigger': 1,\n",
       "         'ночую': 3,\n",
       "         'http://t.co/n9nkesxwpm': 1,\n",
       "         'как': 6631,\n",
       "         'планировать': 39,\n",
       "         'провести': 141,\n",
       "         'каникулы': 177,\n",
       "         '—': 1325,\n",
       "         'задротить': 2,\n",
       "         'тви': 182,\n",
       "         'смотреть': 1146,\n",
       "         'сериалы': 1,\n",
       "         'учить': 207,\n",
       "         'химию': 3,\n",
       "         'http://t.co/rk4ppo9iy2': 1,\n",
       "         '@4o_takoe': 1,\n",
       "         '@kiviforever': 3,\n",
       "         '@bce_ha_mapc_666': 3,\n",
       "         '@ki_l': 3,\n",
       "         'нииит': 3,\n",
       "         'просить': 150,\n",
       "         'оставайся': 13,\n",
       "         'хороший': 2164,\n",
       "         'сырок': 3,\n",
       "         'прости': 135,\n",
       "         '@yeleleo': 1,\n",
       "         'http://t.co/lzzggnes3': 1,\n",
       "         'm': 72,\n",
       "         'да': 2788,\n",
       "         'судить': 79,\n",
       "         'карта': 77,\n",
       "         'там': 1496,\n",
       "         'прямой': 44,\n",
       "         'участок': 10,\n",
       "         'хрен': 82,\n",
       "         'срежешь': 1,\n",
       "         '=)': 434,\n",
       "         'соответственно': 6,\n",
       "         'место': 382,\n",
       "         'остановка': 52,\n",
       "         'где': 782,\n",
       "         'тип': 149,\n",
       "         'переночевать': 4,\n",
       "         'ахах': 445,\n",
       "         'дома': 539,\n",
       "         'скучный': 228,\n",
       "         '...': 6319,\n",
       "         'никто': 625,\n",
       "         'писать': 868,\n",
       "         'гулять': 251,\n",
       "         'зовёт': 6,\n",
       "         '@skittles_skittl': 1,\n",
       "         'надо': 1618,\n",
       "         'бы': 2386,\n",
       "         'подарить': 201,\n",
       "         'слуховой': 1,\n",
       "         'аппарат': 13,\n",
       "         'мой': 3418,\n",
       "         'пень': 8,\n",
       "         '@grigoraash': 2,\n",
       "         ':*': 1210,\n",
       "         '@dew3l': 2,\n",
       "         'думать': 1133,\n",
       "         'два': 636,\n",
       "         'экзамен': 276,\n",
       "         'удача': 190,\n",
       "         'http://t.co/va6axf74cs': 1,\n",
       "         'аквапарк': 2,\n",
       "         'казань': 26,\n",
       "         'кома': 9,\n",
       "         'но': 4604,\n",
       "         'мне': 5513,\n",
       "         'совершенно': 61,\n",
       "         'понравиться': 249,\n",
       "         'ж': 371,\n",
       "         'больший': 136,\n",
       "         'ожидать': 85,\n",
       "         '@vikaa_green': 6,\n",
       "         'порнуха': 10,\n",
       "         'жи': 28,\n",
       "         '': 9627,\n",
       "         'просто': 1839,\n",
       "         'слишком': 241,\n",
       "         'извращинец': 2,\n",
       "         '@polinamomsen': 1,\n",
       "         'задача': 40,\n",
       "         'физика': 130,\n",
       "         'решаю': 4,\n",
       "         ':-(((': 43,\n",
       "         'пиздос': 3,\n",
       "         'че': 284,\n",
       "         'хз': 67,\n",
       "         'корова': 20,\n",
       "         '33': 37,\n",
       "         'одного': 140,\n",
       "         'бык': 1,\n",
       "         'томный': 3,\n",
       "         'взгляд': 59,\n",
       "         'тело': 72,\n",
       "         'пастух': 1,\n",
       "         'http://t.co/ajxnhtgkif': 1,\n",
       "         'сидеть': 944,\n",
       "         'филармония': 3,\n",
       "         'тут': 1027,\n",
       "         'татарск': 1,\n",
       "         'ничто': 1023,\n",
       "         'понимать': 581,\n",
       "         '¡': 1,\n",
       "         'хотеть': 3302,\n",
       "         '#': 5186,\n",
       "         'жизньболь': 76,\n",
       "         'с': 8764,\n",
       "         'танец': 84,\n",
       "         'со': 889,\n",
       "         'кориццы': 1,\n",
       "         'повылаживать': 1,\n",
       "         'видео': 206,\n",
       "         'это': 6212,\n",
       "         'пздц': 84,\n",
       "         'такоой': 3,\n",
       "         '&': 281,\n",
       "         'gt;&lt': 19,\n",
       "         '@_c_mapca': 2,\n",
       "         '@lemara_jameson': 1,\n",
       "         'лемари': 1,\n",
       "         'устроишь': 2,\n",
       "         ';)': 1313,\n",
       "         '@iamalenkaflo': 1,\n",
       "         'относительно': 5,\n",
       "         '@lyana4': 1,\n",
       "         'добрый': 613,\n",
       "         'утро': 1135,\n",
       "         'ляна': 1,\n",
       "         'отличный': 320,\n",
       "         'нужный': 962,\n",
       "         'козёл': 28,\n",
       "         'кто': 2513,\n",
       "         'чтоб': 262,\n",
       "         'так.-я': 1,\n",
       "         'козёл)-прекрасно': 1,\n",
       "         'http://t.co/blrmaf2kky': 1,\n",
       "         'искренне': 34,\n",
       "         'зима': 346,\n",
       "         'холод': 64,\n",
       "         'снег': 506,\n",
       "         'лицо': 189,\n",
       "         'жуткий': 38,\n",
       "         'ветер': 53,\n",
       "         'самое': 294,\n",
       "         'дурацкий': 55,\n",
       "         'время': 1137,\n",
       "         'побыстрее': 21,\n",
       "         'весна': 68,\n",
       "         'кинотеатр': 25,\n",
       "         'собираться': 181,\n",
       "         'один': 898,\n",
       "         'тот': 1382,\n",
       "         'же': 2876,\n",
       "         'друг': 723,\n",
       "         'другу': 34,\n",
       "         'улыбаются': 2,\n",
       "         '@mouz_ms': 1,\n",
       "         '16': 57,\n",
       "         'письмо': 121,\n",
       "         'ночка': 16,\n",
       "         'воскресение': 110,\n",
       "         'дорогой': 190,\n",
       "         'подругой))))вино': 1,\n",
       "         'пятилетний': 1,\n",
       "         'выдержка': 3,\n",
       "         'плазма': 2,\n",
       "         'любимый': 690,\n",
       "         'фильм': 492,\n",
       "         'горько': 9,\n",
       "         'учиться': 205,\n",
       "         'детки': 5,\n",
       "         'центр': 72,\n",
       "         'подготовка': 36,\n",
       "         'авиационный': 1,\n",
       "         'персонал': 4,\n",
       "         'http://t.co/mgwqczpwpl': 1,\n",
       "         'ебать': 44,\n",
       "         'мозг': 151,\n",
       "         'пораньше': 36,\n",
       "         'загрузить': 18,\n",
       "         '=(': 381,\n",
       "         '@lucyspero': 1,\n",
       "         'ним': 252,\n",
       "         'отмучиться': 2,\n",
       "         'позавчера': 11,\n",
       "         'твой': 682,\n",
       "         'боль': 240,\n",
       "         '@ann_bu': 1,\n",
       "         'горловой': 1,\n",
       "         'инфекция': 4,\n",
       "         'похоже': 65,\n",
       "         ':(((': 748,\n",
       "         'врач': 82,\n",
       "         'вызывать': 35,\n",
       "         '@katepolyakova96': 4,\n",
       "         'оо': 85,\n",
       "         'здорово': 32,\n",
       "         'дядя': 36,\n",
       "         'карелия': 2,\n",
       "         'он': 1982,\n",
       "         'очень': 2143,\n",
       "         'много': 662,\n",
       "         'снимок': 7,\n",
       "         'присылать': 11,\n",
       "         'оттуда': 29,\n",
       "         'вас': 660,\n",
       "         'фанат': 43,\n",
       "         'скинутся': 2,\n",
       "         'swaggacar': 1,\n",
       "         'переехать': 16,\n",
       "         'swaggagrad': 1,\n",
       "         '@katelove94': 1,\n",
       "         'эх': 265,\n",
       "         'держись': 24,\n",
       "         'трудный': 105,\n",
       "         'минута': 362,\n",
       "         'помни': 2,\n",
       "         'одна': 213,\n",
       "         'страдать': 68,\n",
       "         'стать': 609,\n",
       "         'полегче;d': 1,\n",
       "         '@letokot': 1,\n",
       "         'чо': 156,\n",
       "         '3-й': 17,\n",
       "         'работать': 503,\n",
       "         'например': 87,\n",
       "         '@kuzmitch_ru': 2,\n",
       "         'расставил': 1,\n",
       "         'расставить': 3,\n",
       "         'сразу': 322,\n",
       "         'понять': 576,\n",
       "         'вот': 2870,\n",
       "         'пахнуть': 51,\n",
       "         'после': 712,\n",
       "         'ванны)#запахсчастья': 1,\n",
       "         'http://t.co/9mcjwrtwla': 1,\n",
       "         'обед': 63,\n",
       "         'поскольку': 7,\n",
       "         'фёдор': 7,\n",
       "         'ставиться': 12,\n",
       "         'без': 1065,\n",
       "         'мультик': 61,\n",
       "         ':-(': 441,\n",
       "         '@localizator': 3,\n",
       "         'дуже': 12,\n",
       "         'нудный': 8,\n",
       "         'бо': 25,\n",
       "         'нас': 1216,\n",
       "         'екологія': 1,\n",
       "         'придётся': 38,\n",
       "         'вставать': 190,\n",
       "         '6': 229,\n",
       "         'доделать': 8,\n",
       "         'всё': 1152,\n",
       "         'успеть': 182,\n",
       "         'контрошек': 1,\n",
       "         '@olya_ntb2014': 1,\n",
       "         '@aww_1d_69': 1,\n",
       "         'сочувствовать': 43,\n",
       "         'криворукий': 6,\n",
       "         'http://t.co/lrpkkobyn4': 1,\n",
       "         'вспоминать': 117,\n",
       "         'весь': 3401,\n",
       "         'вместе': 244,\n",
       "         'обнимать': 36,\n",
       "         'как.блин': 1,\n",
       "         '@takeabooow': 2,\n",
       "         'идти': 998,\n",
       "         'оба': 72,\n",
       "         'нахуй': 98,\n",
       "         'ваш': 292,\n",
       "         'сонг182': 1,\n",
       "         '@wikyzymynisi': 1,\n",
       "         'бежать': 55,\n",
       "         'автобус': 138,\n",
       "         'водитель': 34,\n",
       "         'оставливается': 1,\n",
       "         'пробегать': 3,\n",
       "         'мимо': 39,\n",
       "         'http://t.co/8wfcd8hpo8': 1,\n",
       "         'для': 1259,\n",
       "         'близкий': 120,\n",
       "         'конец': 300,\n",
       "         '..': 3314,\n",
       "         'череда': 3,\n",
       "         'неприятность': 9,\n",
       "         'сказать': 1119,\n",
       "         'одному': 43,\n",
       "         'скрину': 1,\n",
       "         'рабстола': 1,\n",
       "         'видно': 112,\n",
       "         'бетанский': 1,\n",
       "         'виктим': 1,\n",
       "         'о_о': 331,\n",
       "         'http://t.co/yfnhjrzjud': 1,\n",
       "         'http://t.co/f8ailq9sbq': 1,\n",
       "         'господь': 57,\n",
       "         'ум': 120,\n",
       "         'сойти': 41,\n",
       "         'кошмар': 71,\n",
       "         'бедняжка': 11,\n",
       "         'сука': 166,\n",
       "         'тварь': 20,\n",
       "         'взрослый': 55,\n",
       "         'жизнь': 924,\n",
       "         'когда': 2247,\n",
       "         'марш': 8,\n",
       "         'кричать': 40,\n",
       "         'мама': 803,\n",
       "         'муж': 111,\n",
       "         'шоколадка': 77,\n",
       "         'вкусняшка': 5,\n",
       "         'ишегалды': 1,\n",
       "         'http://t.co/qjurluxpao': 1,\n",
       "         'ненавидеть': 307,\n",
       "         'тебя': 2160,\n",
       "         'вновь': 24,\n",
       "         'любовь': 267,\n",
       "         '\\xa0': 436,\n",
       "         'autofollowback': 47,\n",
       "         'сбежать': 15,\n",
       "         'от': 1757,\n",
       "         'бронхит': 4,\n",
       "         'тае': 6,\n",
       "         'они': 1057,\n",
       "         'настигнуть': 5,\n",
       "         'ненавижу': 5,\n",
       "         '@didenok': 3,\n",
       "         '@slavashpak': 1,\n",
       "         '@zhlznv': 3,\n",
       "         '@stasssavin': 1,\n",
       "         'вы': 986,\n",
       "         'крутой': 529,\n",
       "         'спорить': 33,\n",
       "         '@seme_malik': 1,\n",
       "         '@sweetharry_94': 3,\n",
       "         'понятный': 133,\n",
       "         'вообще': 1422,\n",
       "         'питер': 96,\n",
       "         'жить': 457,\n",
       "         'норма': 228,\n",
       "         '@_amourserge': 1,\n",
       "         'ошибка': 96,\n",
       "         'ахахах': 278,\n",
       "         'ноут': 74,\n",
       "         'шарить': 9,\n",
       "         '@hitafijahul': 2,\n",
       "         'корона': 11,\n",
       "         'голова': 529,\n",
       "         'докажешь': 1,\n",
       "         'какой': 1755,\n",
       "         'коза': 16,\n",
       "         'подъехать': 4,\n",
       "         '....': 1014,\n",
       "         'http://t.co/9yfss4lumq': 1,\n",
       "         '@undead_kaspers': 3,\n",
       "         '@ksuhadead': 3,\n",
       "         'самая': 147,\n",
       "         'охуенный': 42,\n",
       "         'сестрёнка': 19,\n",
       "         'сам': 245,\n",
       "         'сделать': 869,\n",
       "         'обеденный': 2,\n",
       "         'перерыв': 30,\n",
       "         'молодец': 164,\n",
       "         'http://t.co/oc3hhcc5ty': 1,\n",
       "         'блять': 271,\n",
       "         'нога': 295,\n",
       "         'болеть': 773,\n",
       "         'поход': 205,\n",
       "         'ей': 293,\n",
       "         'пиздец': 267,\n",
       "         '“': 273,\n",
       "         '@kkupryashina': 1,\n",
       "         'целовать': 18,\n",
       "         '”': 238,\n",
       "         'оу': 68,\n",
       "         'катя': 69,\n",
       "         'раньше': 266,\n",
       "         'аня': 68,\n",
       "         'http://t.co/5bfflzn0py': 1,\n",
       "         'было': 1323,\n",
       "         'район': 55,\n",
       "         'ирки': 1,\n",
       "         'спина': 74,\n",
       "         'мокрый': 34,\n",
       "         'зад': 22,\n",
       "         'хуои': 1,\n",
       "         'волялись': 2,\n",
       "         '@cute_me_cute': 2,\n",
       "         '@shalaevapolina': 2,\n",
       "         'спасибо': 1238,\n",
       "         'солнышко': 44,\n",
       "         '30stm': 5,\n",
       "         'up': 6,\n",
       "         'in': 21,\n",
       "         'the': 72,\n",
       "         'air': 11,\n",
       "         'o_o': 137,\n",
       "         'блин': 1160,\n",
       "         'подстава!!куда': 1,\n",
       "         'распределить': 2,\n",
       "         '@vladysha94': 4,\n",
       "         'текст': 61,\n",
       "         '9': 200,\n",
       "         'шооке': 2,\n",
       "         '@sochnay45': 3,\n",
       "         '@thisisswag3': 17,\n",
       "         'сладкий': 126,\n",
       "         'сон': 408,\n",
       "         'родный': 78,\n",
       "         'з': 221,\n",
       "         'удаачи': 1,\n",
       "         'верить': 209,\n",
       "         'получится^^': 1,\n",
       "         '\\n\\n': 318,\n",
       "         'зая': 11,\n",
       "         'приятный': 391,\n",
       "         '@timakova01': 4,\n",
       "         'хорошо:3': 2,\n",
       "         'зачёт': 191,\n",
       "         'сдать': 186,\n",
       "         'рад': 272,\n",
       "         'во': 542,\n",
       "         'серёжка': 19,\n",
       "         'потерять': 146,\n",
       "         'боулинг': 12,\n",
       "         'играть': 380,\n",
       "         'ноготь': 49,\n",
       "         'самый': 321,\n",
       "         'большой': 211,\n",
       "         'сломать': 74,\n",
       "         '8': 241,\n",
       "         'снова': 380,\n",
       "         'приходить': 211,\n",
       "         'закончиться': 271,\n",
       "         '@dean_loves_me': 1,\n",
       "         'ахаха': 383,\n",
       "         'вам': 523,\n",
       "         'тупо': 62,\n",
       "         'ранний': 14,\n",
       "         '@dercappucino': 1,\n",
       "         'можно': 833,\n",
       "         'город': 310,\n",
       "         'мю': 7,\n",
       "         'ниче': 52,\n",
       "         'нет': 2231,\n",
       "         '@epic_fake': 2,\n",
       "         'согласись': 2,\n",
       "         'смешной': 173,\n",
       "         'если': 1600,\n",
       "         'вокруг': 79,\n",
       "         'толпа': 19,\n",
       "         'протестовать': 7,\n",
       "         '@a_kershaw_l': 1,\n",
       "         'ну': 3662,\n",
       "         'бляяя': 49,\n",
       "         'зови': 5,\n",
       "         'артемон': 1,\n",
       "         'срочно': 75,\n",
       "         'пусть': 219,\n",
       "         'выручать': 5,\n",
       "         'мужик': 166,\n",
       "         'или': 990,\n",
       "         'дама': 30,\n",
       "         'помощь': 103,\n",
       "         '@lera_sun': 1,\n",
       "         'посмотреть': 580,\n",
       "         'купить': 593,\n",
       "         'сейчас': 1274,\n",
       "         '@azamataka': 6,\n",
       "         'никакой': 179,\n",
       "         'фантазия': 31,\n",
       "         '@artery_blue': 1,\n",
       "         'чай': 201,\n",
       "         'крепкий': 18,\n",
       "         'оставить': 104,\n",
       "         'пакетик': 6,\n",
       "         'кружка': 23,\n",
       "         'забыть': 486,\n",
       "         'косплей': 7,\n",
       "         'разместить': 6,\n",
       "         'geeks': 1,\n",
       "         'empire': 1,\n",
       "         'назвать': 81,\n",
       "         'шикарный': 100,\n",
       "         'задаться': 8,\n",
       "         'ночи-': 1,\n",
       "         '@bars_tv': 1,\n",
       "         'интересный': 468,\n",
       "         'ссылка': 81,\n",
       "         'про': 865,\n",
       "         'провод': 21,\n",
       "         'факт': 66,\n",
       "         'дтп': 21,\n",
       "         'первый': 790,\n",
       "         'январь': 148,\n",
       "         'оливье': 16,\n",
       "         'чувствовать': 310,\n",
       "         ':((': 739,\n",
       "         'дибильная': 4,\n",
       "         'таблица': 16,\n",
       "         'ней': 243,\n",
       "         'везёт': 13,\n",
       "         'ехать': 408,\n",
       "         'сдавать': 108,\n",
       "         'её': 411,\n",
       "         '@_angel_of_lord': 15,\n",
       "         'стол': 99,\n",
       "         'моя': 25,\n",
       "         'больная': 1,\n",
       "         'строит': 1,\n",
       "         'приличные': 1,\n",
       "         'картинки': 1,\n",
       "         'уме': 3,\n",
       "         'dd': 346,\n",
       "         'жаркийчетверг': 3,\n",
       "         'зарелизили': 2,\n",
       "         'новый': 1533,\n",
       "         'версия': 48,\n",
       "         'cms': 3,\n",
       "         'скоро': 545,\n",
       "         'наш': 735,\n",
       "         'саппорт': 6,\n",
       "         'тим': 7,\n",
       "         'посыпятся': 1,\n",
       "         'куча': 155,\n",
       "         'багрепортов': 1,\n",
       "         'запрос': 13,\n",
       "         'о': 1477,\n",
       "         'помоищ': 1,\n",
       "         '%': 273,\n",
       "         '@ravenheart': 1,\n",
       "         'использовать': 42,\n",
       "         'любой': 140,\n",
       "         'ситуация': 84,\n",
       "         'сколько': 313,\n",
       "         'подписчик': 29,\n",
       "         'оказываться': 140,\n",
       "         'урааа:3': 1,\n",
       "         'http://t.co/kcphx6c81': 1,\n",
       "         't': 48,\n",
       "         '@maria_holms': 1,\n",
       "         'пара': 442,\n",
       "         'история': 278,\n",
       "         'странный': 214,\n",
       "         'вывод': 43,\n",
       "         '@donearchi': 1,\n",
       "         '@luck_close': 1,\n",
       "         '@karinaakuz': 1,\n",
       "         '@dashaaivanovaa': 1,\n",
       "         '@danilkalinin18': 1,\n",
       "         'пойти': 924,\n",
       "         'наряжать': 26,\n",
       "         'маленький': 284,\n",
       "         'ёлка': 269,\n",
       "         'фото': 266,\n",
       "         '@biankanumber1': 4,\n",
       "         'http://t.co/t5bb3y5k0u': 1,\n",
       "         '@tvshigee1': 2,\n",
       "         'өө': 6,\n",
       "         'би': 37,\n",
       "         'амжихгүй': 1,\n",
       "         'ээ': 19,\n",
       "         '2т': 1,\n",
       "         'овог': 1,\n",
       "         'нэрээ': 1,\n",
       "         'хэлчий': 1,\n",
       "         'тэхүү': 1,\n",
       "         'залуусаа': 1,\n",
       "         '@turiindaisan': 3,\n",
       "         '@m_simonyan': 4,\n",
       "         'поясните': 1,\n",
       "         'плз': 8,\n",
       "         'лежачий': 2,\n",
       "         'кремль': 9,\n",
       "         'бить': 53,\n",
       "         'риа': 17,\n",
       "         'q': 87,\n",
       "         'зал': 68,\n",
       "         'ходить': 457,\n",
       "         'порядок': 65,\n",
       "         'привести': 35,\n",
       "         'уважаю': 1,\n",
       "         'красавица': 25,\n",
       "         'a': 107,\n",
       "         'http://t.co/2zembmwzsn': 1,\n",
       "         'video': 8,\n",
       "         'cool': 3,\n",
       "         'respect': 3,\n",
       "         'funny': 3,\n",
       "         '/': 641,\n",
       "         'ухахахаха': 1,\n",
       "         'жесть!крутота': 1,\n",
       "         'http://t.co/fyjdntqjhn': 1,\n",
       "         'орать': 50,\n",
       "         'огорчай': 2,\n",
       "         'http://t.co/9eqqdgdh3': 1,\n",
       "         'g': 71,\n",
       "         'ночной': 59,\n",
       "         'танюша': 6,\n",
       "         'http://t.co/1w0bx3e94w': 1,\n",
       "         'самокритиковать': 1,\n",
       "         'милый': 273,\n",
       "         'обсуждаться': 4,\n",
       "         '@maxim_caulfield': 1,\n",
       "         'телек': 36,\n",
       "         'пытаться': 184,\n",
       "         'тщетно.6не': 1,\n",
       "         'xxx': 26,\n",
       "         'сексуальный': 12,\n",
       "         'расстройство': 14,\n",
       "         'yyy': 9,\n",
       "         'серьёзный': 48,\n",
       "         'серьезней': 1,\n",
       "         'некуда': 28,\n",
       "         'давать': 533,\n",
       "         'расстраиваться': 29,\n",
       "         'через': 497,\n",
       "         'вокзал': 42,\n",
       "         'бошка': 12,\n",
       "         'раскалываться': 19,\n",
       "         'совсем': 506,\n",
       "         'хотеться': 711,\n",
       "         'никуда': 97,\n",
       "         '@nash_east': 1,\n",
       "         '@alexey_pushkov': 2,\n",
       "         'коррупция': 14,\n",
       "         'десять': 33,\n",
       "         'выгодный': 4,\n",
       "         'заказ': 24,\n",
       "         'кавказ': 6,\n",
       "         'пилит': 2,\n",
       "         'мы': 1921,\n",
       "         'уйти': 257,\n",
       "         'другую': 32,\n",
       "         'сфера': 8,\n",
       "         'лень': 177,\n",
       "         'ли))вот': 1,\n",
       "         'ленивый': 50,\n",
       "         '@julia_tamim': 1,\n",
       "         'ха': 75,\n",
       "         'ставить': 122,\n",
       "         'будильник': 42,\n",
       "         'откуда': 88,\n",
       "         'збс': 90,\n",
       "         'хавать': 4,\n",
       "         'ахаххахахаахахах': 1,\n",
       "         'обижался': 1,\n",
       "         'справиться': 14,\n",
       "         'свой': 1628,\n",
       "         'внутренний': 25,\n",
       "         'мир': 305,\n",
       "         'почти': 359,\n",
       "         'под': 520,\n",
       "         'сомнение': 7,\n",
       "         'вера': 24,\n",
       "         'прибавилось': 3,\n",
       "         '@maidly': 1,\n",
       "         '@marryechelon13': 1,\n",
       "         'приезжать': 87,\n",
       "         'погуляеем': 1,\n",
       "         'сс': 21,\n",
       "         'лето': 234,\n",
       "         'ждать': 740,\n",
       "         'гость': 114,\n",
       "         'тысеча': 1,\n",
       "         'праздник': 270,\n",
       "         'настоящий': 134,\n",
       "         'б': 204,\n",
       "         'песня': 406,\n",
       "         'лия': 9,\n",
       "         'х': 358,\n",
       "         'божественный': 6,\n",
       "         'многие': 66,\n",
       "         'ее': 408,\n",
       "         'превозносят': 1,\n",
       "         'кококо': 4,\n",
       "         'корь': 1,\n",
       "         'лиякори': 1,\n",
       "         '@samosad': 1,\n",
       "         'пробегу': 1,\n",
       "         'следующий': 182,\n",
       "         'пинать': 5,\n",
       "         'бегать': 68,\n",
       "         'http://t.co/zpzzp8lnwn': 1,\n",
       "         'утомиться': 1,\n",
       "         'дико': 50,\n",
       "         'сложный': 179,\n",
       "         '6,5': 1,\n",
       "         'восстановление': 7,\n",
       "         'сила': 185,\n",
       "         'ночь': 800,\n",
       "         '8:05': 1,\n",
       "         'особо': 71,\n",
       "         'умный': 75,\n",
       "         'чтобы': 720,\n",
       "         'восстановить': 20,\n",
       "         'режим': 56,\n",
       "         'аааа': 115,\n",
       "         'убить': 164,\n",
       "         'мальчик': 159,\n",
       "         '@vladikkarp': 1,\n",
       "         '@al_aaleksandra': 1,\n",
       "         'пфф': 13,\n",
       "         'барсук': 1,\n",
       "         'месси': 6,\n",
       "         'хватать': 261,\n",
       "         '@koooorobova': 1,\n",
       "         'безумно': 108,\n",
       "         'простой': 109,\n",
       "         'лчк': 1,\n",
       "         'вернуться': 132,\n",
       "         '@unburied_dead': 1,\n",
       "         'сама': 303,\n",
       "         'потеряться': 17,\n",
       "         'рокер': 4,\n",
       "         'унизить': 4,\n",
       "         '@hotcheburek': 1,\n",
       "         'барабанный': 6,\n",
       "         'дробь': 3,\n",
       "         'ита': 15,\n",
       "         'http://t.co/ibha19ng8': 1,\n",
       "         'плюс': 73,\n",
       "         'безалкогольный': 1,\n",
       "         'пиво': 63,\n",
       "         'жена': 119,\n",
       "         'будет': 1919,\n",
       "         'ругаться': 48,\n",
       "         'нее': 230,\n",
       "         'никак': 212,\n",
       "         'к': 1963,\n",
       "         'любимым': 6,\n",
       "         '@silverunicorn': 1,\n",
       "         'невоспитанный': 1,\n",
       "         'ребёнок': 308,\n",
       "         '@s_bezrukov': 13,\n",
       "         'нижний': 21,\n",
       "         'голый': 22,\n",
       "         'асфальт': 21,\n",
       "         'грустно': 26,\n",
       "         'нам': 510,\n",
       "         'тоже': 1540,\n",
       "         'м': 62,\n",
       "         'безответственность': 1,\n",
       "         'кишеть': 2,\n",
       "         'нем': 70,\n",
       "         'бог': 266,\n",
       "         'из': 2074,\n",
       "         'него': 447,\n",
       "         'вырасти': 34,\n",
       "         'фейкин': 1,\n",
       "         'бол': 24,\n",
       "         'ахахх': 11,\n",
       "         'козерог': 4,\n",
       "         'слышать': 152,\n",
       "         'целый': 216,\n",
       "         'нация': 7,\n",
       "         'пасть': 12,\n",
       "         'мэгготов': 1,\n",
       "         'потом': 791,\n",
       "         'будут': 276,\n",
       "         'везде': 96,\n",
       "         'slipknot': 6,\n",
       "         'живой': 96,\n",
       "         'тд': 27,\n",
       "         'майли': 13,\n",
       "         'говорить': 1003,\n",
       "         'пора': 348,\n",
       "         'распределять': 4,\n",
       "         'http://t.co/f4zqkjdiiz': 1,\n",
       "         'трясёт': 2,\n",
       "         'чё': 84,\n",
       "         'титаник': 4,\n",
       "         '@lelik_lol': 1,\n",
       "         'чинить': 7,\n",
       "         'неполадка': 2,\n",
       "         '@anya_boykospn': 1,\n",
       "         'последний': 637,\n",
       "         'серия': 275,\n",
       "         'улитка': 7,\n",
       "         'чуть': 308,\n",
       "         'сбить': 19,\n",
       "         'машина': 234,\n",
       "         'главный': 306,\n",
       "         'пешеходный': 3,\n",
       "         'переход': 14,\n",
       "         'неё': 122,\n",
       "         'чудо': 84,\n",
       "         '@igor_pravo_iv': 1,\n",
       "         'гошик': 1,\n",
       "         'заболеть': 251,\n",
       "         'зайчик': 18,\n",
       "         'лимон': 21,\n",
       "         'мёд': 28,\n",
       "         'имбирь': 5,\n",
       "         'действительно': 136,\n",
       "         'вирус': 12,\n",
       "         'бесполезный': 14,\n",
       "         'действовать': 30,\n",
       "         'вспомнить': 234,\n",
       "         'рассвет': 12,\n",
       "         'лагодой': 1,\n",
       "         'http://t.co/ewmcgvofqo': 1,\n",
       "         'ох': 208,\n",
       "         'уж': 367,\n",
       "         'долг': 43,\n",
       "         'горло': 117,\n",
       "         'перед': 268,\n",
       "         'нг': 453,\n",
       "         '@lepssukaleps': 1,\n",
       "         'гламурновато': 1,\n",
       "         'печально': 34,\n",
       "         'второй': 442,\n",
       "         'хоббит': 29,\n",
       "         '@sashagagarin': 1,\n",
       "         'январский': 1,\n",
       "         'план': 200,\n",
       "         'акустический': 2,\n",
       "         'концерт': 210,\n",
       "         'группа': 205,\n",
       "         'сансара': 1,\n",
       "         'концертный': 5,\n",
       "         'площадка': 11,\n",
       "         'http://t.co/qjnqyymzwf': 1,\n",
       "         '@goliy_benedikt': 1,\n",
       "         'виноватый': 70,\n",
       "         'дейва?)закидать': 1,\n",
       "         'флудилку': 1,\n",
       "         'фотками': 7,\n",
       "         'depechemode': 2,\n",
       "         'забавный': 58,\n",
       "         'разработчик': 8,\n",
       "         'игра': 265,\n",
       "         'бояться': 349,\n",
       "         'коммьюнити': 1,\n",
       "         'урок': 544,\n",
       "         ...})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wKHM_LU1seA",
    "outputId": "22c2891d-2daa-4c43-8a7d-7b0e4f10d807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "уникальных слов, встретившихся больше 5 раз: 24257\n"
     ]
    }
   ],
   "source": [
    "filtered_vocab = set()\n",
    "\n",
    "for word in vocab:\n",
    "    if vocab[word] > 5:\n",
    "        filtered_vocab.add(word)\n",
    "print('уникальных слов, встретившихся больше 5 раз:', len(filtered_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "BEUhJv5N1seC"
   },
   "outputs": [],
   "source": [
    "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
    "word2id = {'PAD':0}\n",
    "\n",
    "for symbol in filtered_vocab:\n",
    "    word2id[symbol] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "P6J89I9d1seC"
   },
   "outputs": [],
   "source": [
    "#обратный словарь для того, чтобы раскодировать последовательность\n",
    "id2word = {i:word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrzM7MnCQeP_",
    "outputId": "5ecd631f-a97a-438c-b9d2-3f994eb9a806"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obiXRWLt1OZJ"
   },
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "FMs4ZohJ1seI"
   },
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, word2id, DEVICE):\n",
    "        self.dataset = dataset['text'].values\n",
    "        self.word2id = word2id\n",
    "        self.length = dataset.shape[0]\n",
    "        self.target = dataset['type'].values\n",
    "        self.device = DEVICE\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        words = self.dataset[index].split(' ')\n",
    "        ids = torch.LongTensor([self.word2id[word.lower()] for word in words if word.lower() in self.word2id])\n",
    "        y = [self.target[index]]\n",
    "        return ids, y\n",
    "\n",
    "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
    "    # он понадобится для DataLoader во время итерации по батчам\n",
    "        ids, y = list(zip(*batch))\n",
    "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
    "        y = torch.Tensor(y).to(self.device)\n",
    "        return padded_ids, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5Wjfyxar7U1"
   },
   "source": [
    "### создаем итераторы по данным для трейна и теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "8Zl4FxB71seI"
   },
   "outputs": [],
   "source": [
    "train_dataset = TweetsDataset(train_data, word2id, DEVICE)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "xauLXZwQNcCq"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOqAu8X8S36K",
    "outputId": "b0d6b0b8-b334-4bdf-8181-f0b4324d747c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 18])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "aZbD27_A6Nck",
    "outputId": "14d5a1f5-6037-4976-cff2-60f24266ff5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['надо',\n",
       " 'бы',\n",
       " 'перестать',\n",
       " 'лениться',\n",
       " 'одевать',\n",
       " 'ремень',\n",
       " 'на',\n",
       " 'штаны',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2word[int(i)] for i in batch[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tj6Wltq3TfGQ",
    "outputId": "7eb93dea-f5f6-4748-b9db-70bf93b82de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "ZR_upb-_1seJ"
   },
   "outputs": [],
   "source": [
    "val_dataset = TweetsDataset(val_data, word2id, DEVICE)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQlpcUR28V-j",
    "outputId": "70c82a34-0bd9-4968-a2f6-ce2c261e8a19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 21])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(val_iterator))\n",
    "test_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUCT8ayD1seK"
   },
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfVEoM-IZFFe",
    "outputId": "8f421f2e-f59e-4eb1-fc81-1d45ff47da1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3758,  0.5386,  0.6521, -0.4415],\n",
       "         [-0.7028, -0.1305, -0.5057,  0.0310],\n",
       "         [-0.7509, -2.1625, -0.2247,  0.9128],\n",
       "         [-1.2174, -1.6512,  0.0316,  1.0966],\n",
       "         [-0.8049,  0.5391,  0.7198, -0.0848],\n",
       "         [ 1.8776,  2.1683,  1.2538,  0.5440],\n",
       "         [ 0.0085, -0.1087,  1.5129, -0.5891],\n",
       "         [-1.3509, -0.3874,  1.1731,  0.2464]],\n",
       "\n",
       "        [[ 0.1856, -1.4954,  0.5557,  1.5093],\n",
       "         [-0.8962, -0.3355, -0.6238,  0.5596],\n",
       "         [-0.6386, -1.0572,  1.9130, -0.3563],\n",
       "         [ 1.1571, -1.6483, -0.4471, -1.2205],\n",
       "         [-0.1934, -0.1949, -0.5082, -0.5347],\n",
       "         [-0.1518,  0.1906, -0.5117,  0.1265],\n",
       "         [ 0.1161,  0.7506, -0.0486, -0.3525],\n",
       "         [ 1.2139, -1.4054,  1.0630,  0.3576]],\n",
       "\n",
       "        [[-0.8484,  0.0110,  0.0082,  1.3653],\n",
       "         [-0.1811,  0.0562, -0.2274,  0.2589],\n",
       "         [ 0.9427, -0.5422, -0.6181,  1.1976],\n",
       "         [ 0.2369,  0.2588,  0.8247, -0.3301],\n",
       "         [-1.3810,  0.8723,  0.3113,  0.4015],\n",
       "         [-0.1361,  0.5827,  0.1444,  2.9218],\n",
       "         [-1.5234, -0.4633, -0.4599, -0.3906],\n",
       "         [-1.0626, -0.5573,  0.2154, -1.1179]]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = torch.randn(3, 8, 4) #batch_size, num_filters, seq_len\n",
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXXF4sU2ZFy4",
    "outputId": "84b83853-bd68-424a-db1b-71bb2f2a6862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5386,  0.6521],\n",
       "         [-0.1305,  0.0310],\n",
       "         [-0.7509,  0.9128],\n",
       "         [-1.2174,  1.0966],\n",
       "         [ 0.5391,  0.7198],\n",
       "         [ 2.1683,  1.2538],\n",
       "         [ 0.0085,  1.5129],\n",
       "         [-0.3874,  1.1731]],\n",
       "\n",
       "        [[ 0.1856,  1.5093],\n",
       "         [-0.3355,  0.5596],\n",
       "         [-0.6386,  1.9130],\n",
       "         [ 1.1571, -0.4471],\n",
       "         [-0.1934, -0.5082],\n",
       "         [ 0.1906,  0.1265],\n",
       "         [ 0.7506, -0.0486],\n",
       "         [ 1.2139,  1.0630]],\n",
       "\n",
       "        [[ 0.0110,  1.3653],\n",
       "         [ 0.0562,  0.2589],\n",
       "         [ 0.9427,  1.1976],\n",
       "         [ 0.2588,  0.8247],\n",
       "         [ 0.8723,  0.4015],\n",
       "         [ 0.5827,  2.9218],\n",
       "         [-0.4633, -0.3906],\n",
       "         [-0.5573,  0.2154]]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "print(mp(fm).shape)\n",
    "mp(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JZy7ZDlctwM",
    "outputId": "b1696999-a3c0-4262-cb53-41bf8905754d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6521,  0.0310,  0.9128,  1.0966,  0.7198,  2.1683,  1.5129,  1.1731],\n",
       "        [ 1.5093,  0.5596,  1.9130,  1.1571, -0.1934,  0.1906,  0.7506,  1.2139],\n",
       "        [ 1.3653,  0.2589,  1.1976,  0.8247,  0.8723,  2.9218, -0.3906,  0.2154]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.max(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "NdV7oSIc1seK"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
    "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
    "        self.concat_bigrams = nn.Conv1d(in_channels=180, out_channels=100, kernel_size=2, padding='same')\n",
    "        #self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text):\n",
    "        #batch_size x seq_len\n",
    "        embedded = self.embedding(text)\n",
    "        #batch_size x seq_len x embedding_dim\n",
    "        embedded = embedded.transpose(1,2)\n",
    "        #batch_size x embedding_dim x seq_len\n",
    "        #feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded))))\n",
    "        #batch_size x filter_count2 x seq_len* \n",
    "        #feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
    "        #batch_size x filter_count3 x seq_len*\n",
    "        feature_map_bigrams = self.relu(self.bigrams(embedded))\n",
    "        # batch_size x filter_count3 x seq_len*\n",
    "        feature_map_trigrams = self.relu(self.trigrams(embedded))\n",
    "\n",
    "        #pooling1 = feature_map_bigrams.max(2)[0] \n",
    "        # batch_size x filter_count2\n",
    "        #pooling2 = feature_map_trigrams.max(2)[0]\n",
    "        # batch_size x filter_count3\n",
    "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
    "        feature_map_concat = self.relu(self.concat_bigrams(concat))\n",
    "        pooling = feature_map_concat.max(2)[0]\n",
    "        # batch _size x (filter_count2 + filter_count3)\n",
    "        logits = self.hidden(pooling)\n",
    "        logits = self.out(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFQ-WEY4cxkv",
    "outputId": "d8a84c76-aad6-4529-a0bf-0eba0fe18e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23932, 22433,  4569,  ...,     0,     0,     0],\n",
      "        [ 9701, 23825, 23994,  ...,     0,     0,     0],\n",
      "        [15987, 21681, 13883,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1182,  4899,  7538,  ...,     0,     0,     0],\n",
      "        [10795, 23083, 19601,  ...,     0,     0,     0],\n",
      "        [22433, 16156, 12510,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "batch, y = next(iter(train_iterator))\n",
    "batch, y = batch.to(device='cpu'), y.to(device='cpu')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23932, 22433,  4569,  ...,     0,     0,     0],\n",
      "        [ 9701, 23825, 23994,  ...,     0,     0,     0],\n",
      "        [15987, 21681, 13883,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1182,  4899,  7538,  ...,     0,     0,     0],\n",
      "        [10795, 23083, 19601,  ...,     0,     0,     0],\n",
      "        [22433, 16156, 12510,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSmsX_fg4LcO",
    "outputId": "3fd4eec0-cf9c-4ad0-a016-d9301b1cd055"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHc9E_96dJBH",
    "outputId": "906c52ad-5e04-4586-990f-b5e9adc6edbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4779],\n",
      "        [0.4804],\n",
      "        [0.4916],\n",
      "        [0.4732],\n",
      "        [0.4773],\n",
      "        [0.4720],\n",
      "        [0.4712],\n",
      "        [0.4824],\n",
      "        [0.4654],\n",
      "        [0.4700],\n",
      "        [0.4786],\n",
      "        [0.4701],\n",
      "        [0.4787],\n",
      "        [0.4676],\n",
      "        [0.4640],\n",
      "        [0.4858],\n",
      "        [0.4889],\n",
      "        [0.4817],\n",
      "        [0.4810],\n",
      "        [0.4859],\n",
      "        [0.4617],\n",
      "        [0.4587],\n",
      "        [0.5062],\n",
      "        [0.4630],\n",
      "        [0.4867],\n",
      "        [0.5022],\n",
      "        [0.4781],\n",
      "        [0.4727],\n",
      "        [0.4823],\n",
      "        [0.4772],\n",
      "        [0.4818],\n",
      "        [0.4790],\n",
      "        [0.4877],\n",
      "        [0.4817],\n",
      "        [0.4588],\n",
      "        [0.4920],\n",
      "        [0.4981],\n",
      "        [0.4795],\n",
      "        [0.4831],\n",
      "        [0.4802],\n",
      "        [0.4645],\n",
      "        [0.4805],\n",
      "        [0.4938],\n",
      "        [0.4624],\n",
      "        [0.4641],\n",
      "        [0.4721],\n",
      "        [0.4698],\n",
      "        [0.4552],\n",
      "        [0.4921],\n",
      "        [0.4669],\n",
      "        [0.5022],\n",
      "        [0.4781],\n",
      "        [0.4667],\n",
      "        [0.4669],\n",
      "        [0.4712],\n",
      "        [0.4860],\n",
      "        [0.5135],\n",
      "        [0.4694],\n",
      "        [0.4986],\n",
      "        [0.4857],\n",
      "        [0.4921],\n",
      "        [0.4584],\n",
      "        [0.4732],\n",
      "        [0.4950]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = CNN(len(id2word), 64)\n",
    "output = model(batch)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBWdfROdQK5v",
    "outputId": "93408385-6f5c-45f7-e6ed-d2feafd8d6c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6879, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "loss(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q73ELWxPzjtZ",
    "outputId": "5a18feb0-b932-407f-8a2e-02b1fa41e8b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2286)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(output, y.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYz0OzYT1vt1"
   },
   "source": [
    "### training loop, логика обучения и валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8sZOK7Kvk7o"
   },
   "source": [
    "теперь нам нужны функции для обучения и валидации,\n",
    "каждый вызов функции - одна эпоха обучения \n",
    "\n",
    "За одну эпоху нам надо для каждого батча:\n",
    "\n",
    "-- применить к нему модель, \n",
    "\n",
    "-- посчитать значение функции потерь, \n",
    "\n",
    "-- посчитать градиенты,\n",
    "\n",
    "-- обновить веса (параметры модели)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "BVKQzPPI1seJ"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
    "\n",
    "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
    "\n",
    "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
    "        optimizer.zero_grad()  #обнуляем градиенты\n",
    "        preds = model(texts)  #прогоняем данные через модель\n",
    "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
    "        loss.backward() #считаем градиенты  \n",
    "        optimizer.step() #обновляем веса \n",
    "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
    "        if not (i + 1) % int(len(iterator)/5):\n",
    "            print(f'Train loss: {epoch_loss/i}')      \n",
    "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "EPfO7p9x1seK"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        for i, (texts, ys) in enumerate(iterator):   \n",
    "            preds = model(texts)  # делаем предсказания на тесте\n",
    "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
    "            epoch_loss += loss.item()\n",
    "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
    "            epoch_metric += batch_metric\n",
    "\n",
    "            if not (i + 1) % int(len(iterator)/5):\n",
    "                print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
    "        \n",
    "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRoSupMNQsvF"
   },
   "source": [
    "### инициализируем модель, задаем оптимизатор и функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "WO35IZES1seL"
   },
   "outputs": [],
   "source": [
    "model = CNN(len(word2id), 64)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.BCELoss()  \n",
    "\n",
    "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kHbVX0y1seL"
   },
   "source": [
    "### запуск обучения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boMAjlVM1seM",
    "outputId": "d2925b1a-08cc-447a-fa61-0199f7575432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting Epoch 0\n",
      "Training...\n",
      "Train loss: 0.5863362121296105\n",
      "Train loss: 0.5862874459011763\n",
      "Train loss: 0.5864428120507362\n",
      "Train loss: 0.5851654145147657\n",
      "Train loss: 0.5845411649809749\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.5446375540811638, Val f1: 0.7308131456375122\n",
      "Val loss: 0.5448338473996809, Val f1: 0.7295262217521667\n",
      "Val loss: 0.5460332598747727, Val f1: 0.7285375595092773\n",
      "Val loss: 0.5456135751381825, Val f1: 0.727861762046814\n",
      "Val loss: 0.5459026695286077, Val f1: 0.7277320623397827\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.6014044724128864, Val f1: 0.674317479133606\n",
      "Val loss: 0.5990889935695817, Val f1: 0.6761571764945984\n",
      "Val loss: 0.5987235719946737, Val f1: 0.675381064414978\n",
      "Val loss: 0.5998692731501648, Val f1: 0.6736787557601929\n",
      "Val loss: 0.6000124617686855, Val f1: 0.6718292832374573\n",
      "\n",
      "starting Epoch 1\n",
      "Training...\n",
      "Train loss: 0.5474280430162085\n",
      "Train loss: 0.549624158270348\n",
      "Train loss: 0.5503673835563718\n",
      "Train loss: 0.5488493047184463\n",
      "Train loss: 0.5487719936764407\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.49978373218946354, Val f1: 0.7566549777984619\n",
      "Val loss: 0.5043662680184237, Val f1: 0.7515819668769836\n",
      "Val loss: 0.5045302228896859, Val f1: 0.7520325183868408\n",
      "Val loss: 0.5040597760391806, Val f1: 0.7517990469932556\n",
      "Val loss: 0.504724561190816, Val f1: 0.7519832253456116\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.608833744790819, Val f1: 0.6745814085006714\n",
      "Val loss: 0.6033382954632664, Val f1: 0.6768693327903748\n",
      "Val loss: 0.6026267625336565, Val f1: 0.6759020090103149\n",
      "Val loss: 0.6027303765601655, Val f1: 0.6773654222488403\n",
      "Val loss: 0.6030455715350795, Val f1: 0.6750386357307434\n",
      "\n",
      "starting Epoch 2\n",
      "Training...\n",
      "Train loss: 0.5104842265703582\n",
      "Train loss: 0.5097583884193051\n",
      "Train loss: 0.5112986702298064\n",
      "Train loss: 0.5128723026035789\n",
      "Train loss: 0.513493600922555\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.4652894834310806, Val f1: 0.7928791046142578\n",
      "Val loss: 0.4627570595334752, Val f1: 0.7935667037963867\n",
      "Val loss: 0.46319744263658946, Val f1: 0.7927545309066772\n",
      "Val loss: 0.4641756857391438, Val f1: 0.7915027737617493\n",
      "Val loss: 0.4639176294789564, Val f1: 0.7915614247322083\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.6145767342161249, Val f1: 0.6982536315917969\n",
      "Val loss: 0.6102116925030177, Val f1: 0.6977136135101318\n",
      "Val loss: 0.6090115469855231, Val f1: 0.6964144706726074\n",
      "Val loss: 0.6091139126109374, Val f1: 0.6967270970344543\n",
      "Val loss: 0.6099128626583251, Val f1: 0.6954121589660645\n",
      "\n",
      "starting Epoch 3\n",
      "Training...\n",
      "Train loss: 0.46877141758744567\n",
      "Train loss: 0.470403944237441\n",
      "Train loss: 0.47281764696651946\n",
      "Train loss: 0.4747815926823952\n",
      "Train loss: 0.4761580593812615\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.4190598865386745, Val f1: 0.8178760409355164\n",
      "Val loss: 0.4195850435215207, Val f1: 0.8158726692199707\n",
      "Val loss: 0.41923307568859997, Val f1: 0.8147785067558289\n",
      "Val loss: 0.4203668310998058, Val f1: 0.8138394355773926\n",
      "Val loss: 0.4198700856903325, Val f1: 0.8139093518257141\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.6355428446222234, Val f1: 0.6850345730781555\n",
      "Val loss: 0.6285724123025732, Val f1: 0.6870357990264893\n",
      "Val loss: 0.627917354464238, Val f1: 0.6835674047470093\n",
      "Val loss: 0.6272238228970909, Val f1: 0.6866282224655151\n",
      "Val loss: 0.6272911548790068, Val f1: 0.6853942275047302\n",
      "\n",
      "starting Epoch 4\n",
      "Training...\n",
      "Train loss: 0.4278956221698395\n",
      "Train loss: 0.43155370291476974\n",
      "Train loss: 0.4340998443067806\n",
      "Train loss: 0.43637586070995954\n",
      "Train loss: 0.4388442625104283\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.3824522867523876, Val f1: 0.8334256410598755\n",
      "Val loss: 0.38230767237700625, Val f1: 0.8334012031555176\n",
      "Val loss: 0.38154514880818874, Val f1: 0.8334008455276489\n",
      "Val loss: 0.38081176321608634, Val f1: 0.8334828019142151\n",
      "Val loss: 0.38032064328276244, Val f1: 0.8333329558372498\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.6559330311086443, Val f1: 0.6864824295043945\n",
      "Val loss: 0.6505248251876268, Val f1: 0.6835296750068665\n",
      "Val loss: 0.6510507998946844, Val f1: 0.680690348148346\n",
      "Val loss: 0.6523821124494844, Val f1: 0.6803123354911804\n",
      "Val loss: 0.6533629546635983, Val f1: 0.6789906024932861\n",
      "\n",
      "starting Epoch 5\n",
      "Training...\n",
      "Train loss: 0.38977424129568783\n",
      "Train loss: 0.3896098442616001\n",
      "Train loss: 0.39302455841652884\n",
      "Train loss: 0.39695249727825477\n",
      "Train loss: 0.3999933142190306\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.33847647393981467, Val f1: 0.8557977080345154\n",
      "Val loss: 0.33898244511147246, Val f1: 0.8560426235198975\n",
      "Val loss: 0.3389667505604336, Val f1: 0.8548821806907654\n",
      "Val loss: 0.3388757791347846, Val f1: 0.8551194071769714\n",
      "Val loss: 0.3395402586045033, Val f1: 0.8543031215667725\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.6946433601556001, Val f1: 0.6580926179885864\n",
      "Val loss: 0.6897774613651403, Val f1: 0.6572667360305786\n",
      "Val loss: 0.6883514061460331, Val f1: 0.6556496024131775\n",
      "Val loss: 0.6897095081775447, Val f1: 0.657838761806488\n",
      "Val loss: 0.6894859091786116, Val f1: 0.6565181612968445\n",
      "\n",
      "starting Epoch 6\n",
      "Training...\n",
      "Train loss: 0.3453464812128306\n",
      "Train loss: 0.3495934430629976\n",
      "Train loss: 0.355004850692626\n",
      "Train loss: 0.3589565924775046\n",
      "Train loss: 0.3618964879343241\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.29777492316116705, Val f1: 0.8765458464622498\n",
      "Val loss: 0.29758206870973386, Val f1: 0.8760553002357483\n",
      "Val loss: 0.29704588954622974, Val f1: 0.8762086629867554\n",
      "Val loss: 0.29780814763397073, Val f1: 0.8761877417564392\n",
      "Val loss: 0.2978818380632819, Val f1: 0.87592613697052\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.7349281147674278, Val f1: 0.6749870181083679\n",
      "Val loss: 0.7311137145955624, Val f1: 0.6728633642196655\n",
      "Val loss: 0.7295986965335265, Val f1: 0.6714258193969727\n",
      "Val loss: 0.7322413131567216, Val f1: 0.6709025502204895\n",
      "Val loss: 0.7346594410897705, Val f1: 0.6681795716285706\n",
      "\n",
      "starting Epoch 7\n",
      "Training...\n",
      "Train loss: 0.30290750128206734\n",
      "Train loss: 0.30914719548368236\n",
      "Train loss: 0.3161138712407916\n",
      "Train loss: 0.32038670535984815\n",
      "Train loss: 0.32360313301281446\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.26072485482395796, Val f1: 0.8987932801246643\n",
      "Val loss: 0.2598207745340563, Val f1: 0.8988714814186096\n",
      "Val loss: 0.2597178076451109, Val f1: 0.8979983329772949\n",
      "Val loss: 0.2591333488566103, Val f1: 0.8973667025566101\n",
      "Val loss: 0.25948666492113315, Val f1: 0.897033154964447\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.7892078404073362, Val f1: 0.6791065335273743\n",
      "Val loss: 0.7871564578306192, Val f1: 0.6761248111724854\n",
      "Val loss: 0.788145096720876, Val f1: 0.6753193736076355\n",
      "Val loss: 0.7900944659503565, Val f1: 0.6770257949829102\n",
      "Val loss: 0.7914626643127475, Val f1: 0.6751708984375\n",
      "\n",
      "starting Epoch 8\n",
      "Training...\n",
      "Train loss: 0.2654705034685751\n",
      "Train loss: 0.2718989381729732\n",
      "Train loss: 0.27798252423689757\n",
      "Train loss: 0.2834220042101308\n",
      "Train loss: 0.28804762317753047\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.23226036974039904, Val f1: 0.9118791222572327\n",
      "Val loss: 0.23031553786745818, Val f1: 0.9113667607307434\n",
      "Val loss: 0.23047036745789157, Val f1: 0.9110483527183533\n",
      "Val loss: 0.2310898056067612, Val f1: 0.9103640913963318\n",
      "Val loss: 0.2305238482157316, Val f1: 0.9109799861907959\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.8551022790096424, Val f1: 0.6871705055236816\n",
      "Val loss: 0.854769847819726, Val f1: 0.6849274635314941\n",
      "Val loss: 0.8534515623817865, Val f1: 0.682333767414093\n",
      "Val loss: 0.8567003596773025, Val f1: 0.6834985017776489\n",
      "Val loss: 0.8599206534418687, Val f1: 0.6821092963218689\n",
      "\n",
      "starting Epoch 9\n",
      "Training...\n",
      "Train loss: 0.23043031722472163\n",
      "Train loss: 0.23690947586078248\n",
      "Train loss: 0.24487144801610078\n",
      "Train loss: 0.2513383837860337\n",
      "Train loss: 0.2559899750962447\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.21602557715014778, Val f1: 0.916545033454895\n",
      "Val loss: 0.2144371440127698, Val f1: 0.9166605472564697\n",
      "Val loss: 0.21302016643253532, Val f1: 0.9171991348266602\n",
      "Val loss: 0.2127336444275712, Val f1: 0.9170513153076172\n",
      "Val loss: 0.21240852791308126, Val f1: 0.9172113537788391\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.9296190257425662, Val f1: 0.6933336853981018\n",
      "Val loss: 0.9247949798608618, Val f1: 0.6898250579833984\n",
      "Val loss: 0.928153824381512, Val f1: 0.687055766582489\n",
      "Val loss: 0.9303559997353984, Val f1: 0.6867528557777405\n",
      "Val loss: 0.933534916221481, Val f1: 0.6858766078948975\n",
      "\n",
      "starting Epoch 10\n",
      "Training...\n",
      "Train loss: 0.2031919224345816\n",
      "Train loss: 0.2080703695729581\n",
      "Train loss: 0.21284607754841775\n",
      "Train loss: 0.2179386723548078\n",
      "Train loss: 0.22311768442249597\n",
      "\n",
      "Evaluating on train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1715174111987832, Val f1: 0.9368208050727844\n",
      "Val loss: 0.17127656976473496, Val f1: 0.9354495406150818\n",
      "Val loss: 0.1704082389245455, Val f1: 0.9358892440795898\n",
      "Val loss: 0.17089399699514662, Val f1: 0.9351230263710022\n",
      "Val loss: 0.17133508427459787, Val f1: 0.9347357749938965\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 0.9867222501171959, Val f1: 0.6724744439125061\n",
      "Val loss: 0.9865940377061218, Val f1: 0.6624783277511597\n",
      "Val loss: 0.9859158103091125, Val f1: 0.6600662469863892\n",
      "Val loss: 0.9958581334429332, Val f1: 0.661040723323822\n",
      "Val loss: 0.9987083217444581, Val f1: 0.6589279174804688\n",
      "\n",
      "starting Epoch 11\n",
      "Training...\n",
      "Train loss: 0.1759891380243196\n",
      "Train loss: 0.18092926192668177\n",
      "Train loss: 0.18685859507659144\n",
      "Train loss: 0.19242905476012576\n",
      "Train loss: 0.19693493760995945\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.15634765133701567, Val f1: 0.9423356652259827\n",
      "Val loss: 0.1569295764656111, Val f1: 0.9412312507629395\n",
      "Val loss: 0.1572877761955929, Val f1: 0.940460205078125\n",
      "Val loss: 0.1571778395259946, Val f1: 0.9399675726890564\n",
      "Val loss: 0.1566915435382963, Val f1: 0.9399353265762329\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.0908898371237297, Val f1: 0.6837690472602844\n",
      "Val loss: 1.0886354767528408, Val f1: 0.6811777353286743\n",
      "Val loss: 1.0898889836280878, Val f1: 0.6781550645828247\n",
      "Val loss: 1.0983925503481378, Val f1: 0.67903733253479\n",
      "Val loss: 1.0975470058345653, Val f1: 0.6779959797859192\n",
      "\n",
      "starting Epoch 12\n",
      "Training...\n",
      "Train loss: 0.1514778453444313\n",
      "Train loss: 0.1590121442340486\n",
      "Train loss: 0.1632114789844548\n",
      "Train loss: 0.16779401677715125\n",
      "Train loss: 0.1718294194672331\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.12934833358822054, Val f1: 0.9536035060882568\n",
      "Val loss: 0.12883614635755938, Val f1: 0.9528242945671082\n",
      "Val loss: 0.12835517726582304, Val f1: 0.952501118183136\n",
      "Val loss: 0.1294407503879301, Val f1: 0.9516735076904297\n",
      "Val loss: 0.1294733067588742, Val f1: 0.951442301273346\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.173768863413069, Val f1: 0.6667070984840393\n",
      "Val loss: 1.1681105875001183, Val f1: 0.6626762747764587\n",
      "Val loss: 1.1663915514945984, Val f1: 0.6618521809577942\n",
      "Val loss: 1.1719022923411586, Val f1: 0.6612142324447632\n",
      "Val loss: 1.1781609323484732, Val f1: 0.6603127717971802\n",
      "\n",
      "starting Epoch 13\n",
      "Training...\n",
      "Train loss: 0.13467299083172174\n",
      "Train loss: 0.13637427650563727\n",
      "Train loss: 0.14130710747724523\n",
      "Train loss: 0.14696123240277933\n",
      "Train loss: 0.1518609497336535\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.1116490599551544, Val f1: 0.9607005715370178\n",
      "Val loss: 0.11200998040937608, Val f1: 0.9592063426971436\n",
      "Val loss: 0.11090195064000438, Val f1: 0.9591996073722839\n",
      "Val loss: 0.11116196820962847, Val f1: 0.9589197039604187\n",
      "Val loss: 0.11109334584970683, Val f1: 0.958679735660553\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.2650137857154564, Val f1: 0.6617337465286255\n",
      "Val loss: 1.2556591348454522, Val f1: 0.6597889065742493\n",
      "Val loss: 1.2552292825841787, Val f1: 0.6584592461585999\n",
      "Val loss: 1.2685536101158592, Val f1: 0.6587088108062744\n",
      "Val loss: 1.2735209988213427, Val f1: 0.6569900512695312\n",
      "\n",
      "starting Epoch 14\n",
      "Training...\n",
      "Train loss: 0.11512646677034367\n",
      "Train loss: 0.11805662055787403\n",
      "Train loss: 0.12499175117069658\n",
      "Train loss: 0.13003557026798085\n",
      "Train loss: 0.13329288982782805\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.09981459995381177, Val f1: 0.962354838848114\n",
      "Val loss: 0.10093521274420249, Val f1: 0.9614371657371521\n",
      "Val loss: 0.10120764188354044, Val f1: 0.9610511064529419\n",
      "Val loss: 0.10003671876722385, Val f1: 0.9617888927459717\n",
      "Val loss: 0.09961711410117352, Val f1: 0.9621862173080444\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.4163045097280431, Val f1: 0.655410885810852\n",
      "Val loss: 1.4069091420332005, Val f1: 0.6521387100219727\n",
      "Val loss: 1.3997396415223067, Val f1: 0.6505922675132751\n",
      "Val loss: 1.4071348455509867, Val f1: 0.6517190933227539\n",
      "Val loss: 1.4100884164907093, Val f1: 0.6498398780822754\n",
      "\n",
      "starting Epoch 15\n",
      "Training...\n",
      "Train loss: 0.09878167767778753\n",
      "Train loss: 0.10454312438308369\n",
      "Train loss: 0.11030693965934271\n",
      "Train loss: 0.11397189767074607\n",
      "Train loss: 0.11802600877868752\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.10125275622585933, Val f1: 0.9605109691619873\n",
      "Val loss: 0.10112116734930722, Val f1: 0.9593632221221924\n",
      "Val loss: 0.10089688188641238, Val f1: 0.9595131874084473\n",
      "Val loss: 0.10054317746999515, Val f1: 0.9596825838088989\n",
      "Val loss: 0.10003915659852694, Val f1: 0.9599538445472717\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.5526221535823963, Val f1: 0.633545458316803\n",
      "Val loss: 1.5445856709321926, Val f1: 0.6299760937690735\n",
      "Val loss: 1.5410313051221411, Val f1: 0.6293314099311829\n",
      "Val loss: 1.5428314217765906, Val f1: 0.6294774413108826\n",
      "Val loss: 1.5456622430783133, Val f1: 0.6294169425964355\n",
      "\n",
      "starting Epoch 16\n",
      "Training...\n",
      "Train loss: 0.09083087770508884\n",
      "Train loss: 0.09480466495529848\n",
      "Train loss: 0.09844103936135715\n",
      "Train loss: 0.1019771352752645\n",
      "Train loss: 0.10536308428300058\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.0827707171275167, Val f1: 0.9678298830986023\n",
      "Val loss: 0.08215809843507231, Val f1: 0.967704713344574\n",
      "Val loss: 0.08248662552395858, Val f1: 0.9672817587852478\n",
      "Val loss: 0.08192407486866773, Val f1: 0.9672297835350037\n",
      "Val loss: 0.08210459311184588, Val f1: 0.9669846296310425\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.7623938096894158, Val f1: 0.653639554977417\n",
      "Val loss: 1.7456928348629237, Val f1: 0.6513965129852295\n",
      "Val loss: 1.7332598099837433, Val f1: 0.6493220329284668\n",
      "Val loss: 1.7337612102483957, Val f1: 0.6498035788536072\n",
      "Val loss: 1.7192490100860596, Val f1: 0.6475063562393188\n",
      "\n",
      "starting Epoch 17\n",
      "Training...\n",
      "Train loss: 0.08086517415390904\n",
      "Train loss: 0.0852323831501095\n",
      "Train loss: 0.08924166374457702\n",
      "Train loss: 0.09248684196016965\n",
      "Train loss: 0.09623227164378512\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.07147500377897571, Val f1: 0.9738989472389221\n",
      "Val loss: 0.07070468167263654, Val f1: 0.9729844331741333\n",
      "Val loss: 0.0704169570633121, Val f1: 0.9731683731079102\n",
      "Val loss: 0.07085581406957814, Val f1: 0.972920298576355\n",
      "Val loss: 0.07146317266509783, Val f1: 0.972416877746582\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 1.888821009353355, Val f1: 0.6664921641349792\n",
      "Val loss: 1.8888407314395552, Val f1: 0.6603243350982666\n",
      "Val loss: 1.8689922754828994, Val f1: 0.6575242280960083\n",
      "Val loss: 1.88396940826272, Val f1: 0.6574453115463257\n",
      "Val loss: 1.86702933411535, Val f1: 0.6554681658744812\n",
      "\n",
      "starting Epoch 18\n",
      "Training...\n",
      "Train loss: 0.07483734578114154\n",
      "Train loss: 0.07718464797748947\n",
      "Train loss: 0.08040988216684623\n",
      "Train loss: 0.08373992137897474\n",
      "Train loss: 0.08681439999909625\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.06750676636085794, Val f1: 0.9754160046577454\n",
      "Val loss: 0.0660702701400502, Val f1: 0.9751098155975342\n",
      "Val loss: 0.06537837301711587, Val f1: 0.9749170541763306\n",
      "Val loss: 0.06550134036613638, Val f1: 0.974879801273346\n",
      "Val loss: 0.06572787947839506, Val f1: 0.9746785163879395\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 2.065521169591833, Val f1: 0.6692976951599121\n",
      "Val loss: 2.057388528246721, Val f1: 0.6627905368804932\n",
      "Val loss: 2.0606814964104636, Val f1: 0.661318302154541\n",
      "Val loss: 2.076611656510369, Val f1: 0.6627539396286011\n",
      "Val loss: 2.0657120183746662, Val f1: 0.6615499258041382\n",
      "\n",
      "starting Epoch 19\n",
      "Training...\n",
      "Train loss: 0.06833739697287114\n",
      "Train loss: 0.06917389568525113\n",
      "Train loss: 0.07365702292756052\n",
      "Train loss: 0.07804289422431666\n",
      "Train loss: 0.08106936589048114\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.06440674089500165, Val f1: 0.9752235412597656\n",
      "Val loss: 0.06389858626678999, Val f1: 0.9748608469963074\n",
      "Val loss: 0.06257105159775768, Val f1: 0.9752804636955261\n",
      "Val loss: 0.06289800242827592, Val f1: 0.9746400713920593\n",
      "Val loss: 0.06264305445152851, Val f1: 0.9746477007865906\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 2.24869904562279, Val f1: 0.6592294573783875\n",
      "Val loss: 2.234250145864663, Val f1: 0.6545432806015015\n",
      "Val loss: 2.2349057863912654, Val f1: 0.6535831093788147\n",
      "Val loss: 2.24023453383595, Val f1: 0.6542559862136841\n",
      "Val loss: 2.2342098592307327, Val f1: 0.6537160277366638\n",
      "\n",
      "starting Epoch 20\n",
      "Training...\n",
      "Train loss: 0.06548684686628545\n",
      "Train loss: 0.06764219442494042\n",
      "Train loss: 0.06995618313434247\n",
      "Train loss: 0.07234850576862123\n",
      "Train loss: 0.07509696969279282\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.05927888175809405, Val f1: 0.9780828952789307\n",
      "Val loss: 0.05925403395882262, Val f1: 0.9765176177024841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.05938398563337377, Val f1: 0.9761346578598022\n",
      "Val loss: 0.05995616609833788, Val f1: 0.9757260084152222\n",
      "Val loss: 0.06029490250531329, Val f1: 0.9755194187164307\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 2.512988694950386, Val f1: 0.6735525131225586\n",
      "Val loss: 2.5429796400545266, Val f1: 0.6629979014396667\n",
      "Val loss: 2.5449136756269, Val f1: 0.6600173711776733\n",
      "Val loss: 2.5287845100706674, Val f1: 0.6598820686340332\n",
      "Val loss: 2.4957514287621354, Val f1: 0.6587596535682678\n",
      "\n",
      "starting Epoch 21\n",
      "Training...\n",
      "Train loss: 0.059691299537430444\n",
      "Train loss: 0.06091842599007116\n",
      "Train loss: 0.06379504661483082\n",
      "Train loss: 0.06807063967212588\n",
      "Train loss: 0.07046966953855077\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.05281774062024106, Val f1: 0.9796361327171326\n",
      "Val loss: 0.053371453497018065, Val f1: 0.9779148697853088\n",
      "Val loss: 0.05296739960602469, Val f1: 0.9773098230361938\n",
      "Val loss: 0.05339410153748695, Val f1: 0.9770262241363525\n",
      "Val loss: 0.05331162385355651, Val f1: 0.9771751165390015\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 2.5658220330874126, Val f1: 0.6595156788825989\n",
      "Val loss: 2.5960150495226535, Val f1: 0.6523280143737793\n",
      "Val loss: 2.5887858423905525, Val f1: 0.6486461162567139\n",
      "Val loss: 2.6133292539124113, Val f1: 0.6488555669784546\n",
      "Val loss: 2.6094152475316323, Val f1: 0.6488516926765442\n",
      "\n",
      "starting Epoch 22\n",
      "Training...\n",
      "Train loss: 0.053616505534202545\n",
      "Train loss: 0.05777137301935201\n",
      "Train loss: 0.0612008351821473\n",
      "Train loss: 0.06478646329822013\n",
      "Train loss: 0.06694037145988188\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.05342789101276918, Val f1: 0.978914737701416\n",
      "Val loss: 0.053353259767821994, Val f1: 0.9781250357627869\n",
      "Val loss: 0.05399749236248882, Val f1: 0.9774971008300781\n",
      "Val loss: 0.05342411030880088, Val f1: 0.977611780166626\n",
      "Val loss: 0.05383330603492533, Val f1: 0.9776415228843689\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 2.7008957858438847, Val f1: 0.6503086090087891\n",
      "Val loss: 2.6739531469521047, Val f1: 0.645513653755188\n",
      "Val loss: 2.6789286685810043, Val f1: 0.643558144569397\n",
      "Val loss: 2.7007255199644886, Val f1: 0.6435179114341736\n",
      "Val loss: 2.671435733694742, Val f1: 0.6425146460533142\n",
      "\n",
      "starting Epoch 23\n",
      "Training...\n",
      "Train loss: 0.052947986971525085\n",
      "Train loss: 0.05435377315957151\n",
      "Train loss: 0.05807464621064118\n",
      "Train loss: 0.061448902170440224\n",
      "Train loss: 0.06355212360760162\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.06133079985448767, Val f1: 0.975865364074707\n",
      "Val loss: 0.06270911435120254, Val f1: 0.9745805859565735\n",
      "Val loss: 0.06308222406158219, Val f1: 0.9739972352981567\n",
      "Val loss: 0.06243590232416411, Val f1: 0.9742289185523987\n",
      "Val loss: 0.06175957060338039, Val f1: 0.9743194580078125\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.2879658738772073, Val f1: 0.6786316633224487\n",
      "Val loss: 3.2431713578005996, Val f1: 0.6729366779327393\n",
      "Val loss: 3.2439095313484603, Val f1: 0.6710795760154724\n",
      "Val loss: 3.2550668392611692, Val f1: 0.6720286011695862\n",
      "Val loss: 3.248560666072351, Val f1: 0.6709352731704712\n",
      "\n",
      "starting Epoch 24\n",
      "Training...\n",
      "Train loss: 0.0493346282892699\n",
      "Train loss: 0.04972858699639478\n",
      "Train loss: 0.05365583525869895\n",
      "Train loss: 0.05686814918361676\n",
      "Train loss: 0.05948646745833931\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04788138103118077, Val f1: 0.9804006218910217\n",
      "Val loss: 0.04841260244760851, Val f1: 0.9797653555870056\n",
      "Val loss: 0.047904615118271265, Val f1: 0.9798274636268616\n",
      "Val loss: 0.04824735363387687, Val f1: 0.9797053933143616\n",
      "Val loss: 0.04825284638354251, Val f1: 0.9795050024986267\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.230966532230377, Val f1: 0.6683639883995056\n",
      "Val loss: 3.2937183369129786, Val f1: 0.6611934304237366\n",
      "Val loss: 3.296263200498623, Val f1: 0.6604079008102417\n",
      "Val loss: 3.308221509992308, Val f1: 0.6607067584991455\n",
      "Val loss: 3.2955235006707406, Val f1: 0.6596826910972595\n",
      "\n",
      "starting Epoch 25\n",
      "Training...\n",
      "Train loss: 0.045876708587054234\n",
      "Train loss: 0.05044816776741004\n",
      "Train loss: 0.05270505157720952\n",
      "Train loss: 0.05704820567513771\n",
      "Train loss: 0.05883486413907509\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04573386902422592, Val f1: 0.9817955493927002\n",
      "Val loss: 0.04480564465557444, Val f1: 0.9813333749771118\n",
      "Val loss: 0.04487287941922523, Val f1: 0.9812526702880859\n",
      "Val loss: 0.0449274517211828, Val f1: 0.980720043182373\n",
      "Val loss: 0.04473944463858562, Val f1: 0.9806700944900513\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.1654972464949998, Val f1: 0.6625096797943115\n",
      "Val loss: 3.2663735212875027, Val f1: 0.6560932397842407\n",
      "Val loss: 3.266870660483105, Val f1: 0.6520542502403259\n",
      "Val loss: 3.2904506964578153, Val f1: 0.6519360542297363\n",
      "Val loss: 3.2971979696550497, Val f1: 0.6511755585670471\n",
      "\n",
      "starting Epoch 26\n",
      "Training...\n",
      "Train loss: 0.046549566102994266\n",
      "Train loss: 0.0478982359567906\n",
      "Train loss: 0.05140212463362014\n",
      "Train loss: 0.05423134438985661\n",
      "Train loss: 0.05616774028112224\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04559006149978532, Val f1: 0.9812623262405396\n",
      "Val loss: 0.04570384909643487, Val f1: 0.9806109666824341\n",
      "Val loss: 0.044857860994336474, Val f1: 0.9805675745010376\n",
      "Val loss: 0.04447907326498057, Val f1: 0.9807380437850952\n",
      "Val loss: 0.044402934697490286, Val f1: 0.9805856943130493\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.342497562037574, Val f1: 0.6570051908493042\n",
      "Val loss: 3.4061464722306085, Val f1: 0.6511907577514648\n",
      "Val loss: 3.43338716484112, Val f1: 0.6504158973693848\n",
      "Val loss: 3.452955298990176, Val f1: 0.6513722538948059\n",
      "Val loss: 3.453577813852869, Val f1: 0.6507320404052734\n",
      "\n",
      "starting Epoch 27\n",
      "Training...\n",
      "Train loss: 0.04620644514516814\n",
      "Train loss: 0.047506665826500455\n",
      "Train loss: 0.050918306770442165\n",
      "Train loss: 0.052437628836063\n",
      "Train loss: 0.054234767339664855\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.043985591344775914, Val f1: 0.9826698303222656\n",
      "Val loss: 0.0454085118228501, Val f1: 0.9807435870170593\n",
      "Val loss: 0.04490042408075612, Val f1: 0.9805743098258972\n",
      "Val loss: 0.044792249842477386, Val f1: 0.9808384776115417\n",
      "Val loss: 0.044695615435529405, Val f1: 0.9809097647666931\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.7725830519640886, Val f1: 0.6710256338119507\n",
      "Val loss: 3.8102777945599433, Val f1: 0.6673092842102051\n",
      "Val loss: 3.840847796538538, Val f1: 0.6669431328773499\n",
      "Val loss: 3.859404922190292, Val f1: 0.6671015024185181\n",
      "Val loss: 3.842723451587694, Val f1: 0.6658996939659119\n",
      "\n",
      "starting Epoch 28\n",
      "Training...\n",
      "Train loss: 0.04547274617626026\n",
      "Train loss: 0.04700315768362671\n",
      "Train loss: 0.049942090295965115\n",
      "Train loss: 0.05156186581944265\n",
      "Train loss: 0.053057522643585715\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04561802938099817, Val f1: 0.9814042448997498\n",
      "Val loss: 0.04517943311535243, Val f1: 0.9806594848632812\n",
      "Val loss: 0.046035927745217003, Val f1: 0.980078399181366\n",
      "Val loss: 0.04598324805916275, Val f1: 0.9799623489379883\n",
      "Val loss: 0.04581263955363296, Val f1: 0.9800021648406982\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.1056737511246295, Val f1: 0.6722115874290466\n",
      "Val loss: 4.121223175657631, Val f1: 0.6669875383377075\n",
      "Val loss: 4.112858991072278, Val f1: 0.6651900410652161\n",
      "Val loss: 4.148810543847128, Val f1: 0.6659426093101501\n",
      "Val loss: 4.1386293966394465, Val f1: 0.6651090383529663\n",
      "\n",
      "starting Epoch 29\n",
      "Training...\n",
      "Train loss: 0.04519213680470039\n",
      "Train loss: 0.0484101818679201\n",
      "Train loss: 0.05000442920663996\n",
      "Train loss: 0.0516629464284507\n",
      "Train loss: 0.053516909041457075\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04133152791210433, Val f1: 0.9824493527412415\n",
      "Val loss: 0.04103584872389186, Val f1: 0.9820976257324219\n",
      "Val loss: 0.04053407581451553, Val f1: 0.9824225902557373\n",
      "Val loss: 0.04085565330709478, Val f1: 0.9819276928901672\n",
      "Val loss: 0.040903112448004875, Val f1: 0.9818116426467896\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.747286192134575, Val f1: 0.6436346769332886\n",
      "Val loss: 3.775051618194228, Val f1: 0.6427791714668274\n",
      "Val loss: 3.7366869166969376, Val f1: 0.6416493654251099\n",
      "Val loss: 3.7270493372369207, Val f1: 0.6434562802314758\n",
      "Val loss: 3.7287402083666583, Val f1: 0.6416230201721191\n",
      "\n",
      "starting Epoch 30\n",
      "Training...\n",
      "Train loss: 0.04093529537708474\n",
      "Train loss: 0.042275826070642245\n",
      "Train loss: 0.04544288188704468\n",
      "Train loss: 0.04820826461572777\n",
      "Train loss: 0.05000862208627921\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.0481213206265077, Val f1: 0.9800621271133423\n",
      "Val loss: 0.04839037940569205, Val f1: 0.9792162179946899\n",
      "Val loss: 0.048612429236384515, Val f1: 0.9787261486053467\n",
      "Val loss: 0.04868056551321229, Val f1: 0.9783775210380554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.04868147928995549, Val f1: 0.9782818555831909\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 3.574899710107733, Val f1: 0.6359406113624573\n",
      "Val loss: 3.7040185147545874, Val f1: 0.6298350691795349\n",
      "Val loss: 3.7094109332239307, Val f1: 0.6279236674308777\n",
      "Val loss: 3.7363431353814915, Val f1: 0.6292805671691895\n",
      "Val loss: 3.738591043780584, Val f1: 0.6286226511001587\n",
      "\n",
      "starting Epoch 31\n",
      "Training...\n",
      "Train loss: 0.04148168380675508\n",
      "Train loss: 0.04329653475734015\n",
      "Train loss: 0.04661381127428909\n",
      "Train loss: 0.048906091162021166\n",
      "Train loss: 0.05031790626533851\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04042891387735765, Val f1: 0.982399046421051\n",
      "Val loss: 0.04067737252450406, Val f1: 0.9811898469924927\n",
      "Val loss: 0.04083743795971483, Val f1: 0.9811136722564697\n",
      "Val loss: 0.04049721284724325, Val f1: 0.9813019037246704\n",
      "Val loss: 0.04058888459516989, Val f1: 0.9812479019165039\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.449512702447397, Val f1: 0.672086238861084\n",
      "Val loss: 4.515717534561439, Val f1: 0.6640716195106506\n",
      "Val loss: 4.528820992394806, Val f1: 0.6602103114128113\n",
      "Val loss: 4.514596785410114, Val f1: 0.6614791750907898\n",
      "Val loss: 4.495298176872186, Val f1: 0.6594268679618835\n",
      "\n",
      "starting Epoch 32\n",
      "Training...\n",
      "Train loss: 0.03931888200925161\n",
      "Train loss: 0.041530035449645786\n",
      "Train loss: 0.04339200914426605\n",
      "Train loss: 0.04577447389990496\n",
      "Train loss: 0.048135089229237045\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04542839912009491, Val f1: 0.9817628860473633\n",
      "Val loss: 0.04560024660286678, Val f1: 0.9804683327674866\n",
      "Val loss: 0.04558240669076983, Val f1: 0.9803722500801086\n",
      "Val loss: 0.04477801219917763, Val f1: 0.980408251285553\n",
      "Val loss: 0.04452947136597635, Val f1: 0.9804856777191162\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.814599801434411, Val f1: 0.6744032502174377\n",
      "Val loss: 4.9060241571211725, Val f1: 0.6675787568092346\n",
      "Val loss: 4.9403087277084365, Val f1: 0.6670129299163818\n",
      "Val loss: 4.9356043704306884, Val f1: 0.6682415008544922\n",
      "Val loss: 4.911511954836361, Val f1: 0.6669876575469971\n",
      "\n",
      "starting Epoch 33\n",
      "Training...\n",
      "Train loss: 0.04237684356095957\n",
      "Train loss: 0.04340382191365422\n",
      "Train loss: 0.04494717509646818\n",
      "Train loss: 0.04717059208801722\n",
      "Train loss: 0.04821281552504523\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04044022436152346, Val f1: 0.9815105199813843\n",
      "Val loss: 0.03911712782850171, Val f1: 0.9808858036994934\n",
      "Val loss: 0.03976485697056009, Val f1: 0.9805054068565369\n",
      "Val loss: 0.03954892407481583, Val f1: 0.980675995349884\n",
      "Val loss: 0.03908712850772421, Val f1: 0.9809275269508362\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.37952079684646, Val f1: 0.655685305595398\n",
      "Val loss: 4.428715832558945, Val f1: 0.6516857147216797\n",
      "Val loss: 4.498681526512128, Val f1: 0.6489392518997192\n",
      "Val loss: 4.514682090084856, Val f1: 0.6497730612754822\n",
      "Val loss: 4.498780943157044, Val f1: 0.6487795114517212\n",
      "\n",
      "starting Epoch 34\n",
      "Training...\n",
      "Train loss: 0.039695257154968654\n",
      "Train loss: 0.04114113773658125\n",
      "Train loss: 0.04163520845115621\n",
      "Train loss: 0.04408025538992503\n",
      "Train loss: 0.04619105636331702\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03981673287567545, Val f1: 0.9834538698196411\n",
      "Val loss: 0.03912054921893622, Val f1: 0.9829911589622498\n",
      "Val loss: 0.03947477869726148, Val f1: 0.9827855825424194\n",
      "Val loss: 0.03972422209878423, Val f1: 0.9824070930480957\n",
      "Val loss: 0.039394756541000185, Val f1: 0.9825539588928223\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.311893489625719, Val f1: 0.6481481790542603\n",
      "Val loss: 4.375245208669853, Val f1: 0.6437797546386719\n",
      "Val loss: 4.430966232566927, Val f1: 0.6443895101547241\n",
      "Val loss: 4.4175571339345545, Val f1: 0.6457961201667786\n",
      "Val loss: 4.422375533998627, Val f1: 0.6442546248435974\n",
      "\n",
      "starting Epoch 35\n",
      "Training...\n",
      "Train loss: 0.04152640113130219\n",
      "Train loss: 0.04310510390419899\n",
      "Train loss: 0.04335830187047025\n",
      "Train loss: 0.04512619239946601\n",
      "Train loss: 0.045999496872103605\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.038001251938354186, Val f1: 0.9859679937362671\n",
      "Val loss: 0.03845043841484947, Val f1: 0.9837524890899658\n",
      "Val loss: 0.038754371879171835, Val f1: 0.9835817217826843\n",
      "Val loss: 0.037972226757322404, Val f1: 0.9836626648902893\n",
      "Val loss: 0.03810698745404895, Val f1: 0.9833635687828064\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.522180221698902, Val f1: 0.6577466726303101\n",
      "Val loss: 4.585124449536369, Val f1: 0.6548559069633484\n",
      "Val loss: 4.6310327545137895, Val f1: 0.6528957486152649\n",
      "Val loss: 4.599038009265949, Val f1: 0.6519782543182373\n",
      "Val loss: 4.586180580908842, Val f1: 0.6505444049835205\n",
      "\n",
      "starting Epoch 36\n",
      "Training...\n",
      "Train loss: 0.03952590187522239\n",
      "Train loss: 0.04151795109270424\n",
      "Train loss: 0.043772817230443024\n",
      "Train loss: 0.04525504153534896\n",
      "Train loss: 0.04628956165332769\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03849339669693186, Val f1: 0.9840234518051147\n",
      "Val loss: 0.038084395083036564, Val f1: 0.9830812811851501\n",
      "Val loss: 0.03808099693950227, Val f1: 0.9827494621276855\n",
      "Val loss: 0.03775239324614413, Val f1: 0.9830830693244934\n",
      "Val loss: 0.037705897044342566, Val f1: 0.9829491376876831\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.982743906974792, Val f1: 0.6674993634223938\n",
      "Val loss: 5.051713265616075, Val f1: 0.6612011790275574\n",
      "Val loss: 5.099254513109052, Val f1: 0.6585766077041626\n",
      "Val loss: 5.087596221633159, Val f1: 0.6605312824249268\n",
      "Val loss: 5.053361628561203, Val f1: 0.6593496203422546\n",
      "\n",
      "starting Epoch 37\n",
      "Training...\n",
      "Train loss: 0.036922624094452114\n",
      "Train loss: 0.0392026262188841\n",
      "Train loss: 0.042193519412834515\n",
      "Train loss: 0.04310945192983508\n",
      "Train loss: 0.044155585259560655\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03586448738612432, Val f1: 0.9844282865524292\n",
      "Val loss: 0.03649091383440673, Val f1: 0.9832239747047424\n",
      "Val loss: 0.03632273841189139, Val f1: 0.9830317497253418\n",
      "Val loss: 0.03620514983684457, Val f1: 0.9829555749893188\n",
      "Val loss: 0.03566567232178302, Val f1: 0.9831395745277405\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.69077632383064, Val f1: 0.6585278511047363\n",
      "Val loss: 4.773544634620202, Val f1: 0.6523943543434143\n",
      "Val loss: 4.841413667073121, Val f1: 0.6498785018920898\n",
      "Val loss: 4.822900478023191, Val f1: 0.650944173336029\n",
      "Val loss: 4.795013597243313, Val f1: 0.6489332318305969\n",
      "\n",
      "starting Epoch 38\n",
      "Training...\n",
      "Train loss: 0.036142495870624576\n",
      "Train loss: 0.040218665918082104\n",
      "Train loss: 0.042680343281748995\n",
      "Train loss: 0.044728304530395356\n",
      "Train loss: 0.04521318926812792\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.04292863566296113, Val f1: 0.9817323684692383\n",
      "Val loss: 0.042048533760710764, Val f1: 0.9814276695251465\n",
      "Val loss: 0.041172760155580246, Val f1: 0.9814872741699219\n",
      "Val loss: 0.040724189426270124, Val f1: 0.9814982414245605\n",
      "Val loss: 0.040376032253842446, Val f1: 0.9816574454307556\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.461564233567979, Val f1: 0.636599063873291\n",
      "Val loss: 4.48754744732072, Val f1: 0.631070077419281\n",
      "Val loss: 4.5621683811379885, Val f1: 0.6297118663787842\n",
      "Val loss: 4.5599273544209655, Val f1: 0.6313941478729248\n",
      "Val loss: 4.530556579050505, Val f1: 0.6310060024261475\n",
      "\n",
      "starting Epoch 39\n",
      "Training...\n",
      "Train loss: 0.03732520079122499\n",
      "Train loss: 0.03868148686167943\n",
      "Train loss: 0.04110816154692348\n",
      "Train loss: 0.04280669149000649\n",
      "Train loss: 0.044203097319637764\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.038144444797198356, Val f1: 0.9844304323196411\n",
      "Val loss: 0.039462405896567514, Val f1: 0.9829370379447937\n",
      "Val loss: 0.03866784253776364, Val f1: 0.9828488230705261\n",
      "Val loss: 0.03874495251417179, Val f1: 0.9827131032943726\n",
      "Val loss: 0.03858586156952854, Val f1: 0.9828019738197327\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.750553321838379, Val f1: 0.6451703310012817\n",
      "Val loss: 4.762558657744714, Val f1: 0.6404982209205627\n",
      "Val loss: 4.77806091279304, Val f1: 0.6385817527770996\n",
      "Val loss: 4.779532041558464, Val f1: 0.6405150294303894\n",
      "Val loss: 4.7350387725984575, Val f1: 0.6389453411102295\n",
      "\n",
      "starting Epoch 40\n",
      "Training...\n",
      "Train loss: 0.03668634683740878\n",
      "Train loss: 0.03946521052873955\n",
      "Train loss: 0.040929591037571565\n",
      "Train loss: 0.04262390046880011\n",
      "Train loss: 0.04357435675867227\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03499774773763252, Val f1: 0.9861838817596436\n",
      "Val loss: 0.03566667097803068, Val f1: 0.9847970008850098\n",
      "Val loss: 0.03596432578460489, Val f1: 0.9840244650840759\n",
      "Val loss: 0.03614380025542511, Val f1: 0.9837605357170105\n",
      "Val loss: 0.03621664091690717, Val f1: 0.9835540652275085\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.416756321324242, Val f1: 0.6718225479125977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.406999022978258, Val f1: 0.668300211429596\n",
      "Val loss: 5.4583741158466665, Val f1: 0.6653535962104797\n",
      "Val loss: 5.4589602460957805, Val f1: 0.6651381850242615\n",
      "Val loss: 5.456626657797065, Val f1: 0.6640042066574097\n",
      "\n",
      "starting Epoch 41\n",
      "Training...\n",
      "Train loss: 0.03462072047849557\n",
      "Train loss: 0.037357884816973455\n",
      "Train loss: 0.03866018316314066\n",
      "Train loss: 0.039886612671646\n",
      "Train loss: 0.0412446908779664\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.0349692966195332, Val f1: 0.9850504398345947\n",
      "Val loss: 0.03537707741998355, Val f1: 0.9845626950263977\n",
      "Val loss: 0.035924665568992396, Val f1: 0.9839261174201965\n",
      "Val loss: 0.03545725125428895, Val f1: 0.9840405583381653\n",
      "Val loss: 0.03495219104017153, Val f1: 0.9843791723251343\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.10895953796528, Val f1: 0.6555642485618591\n",
      "Val loss: 5.091412399087885, Val f1: 0.6494653820991516\n",
      "Val loss: 5.088722852289823, Val f1: 0.6468754410743713\n",
      "Val loss: 5.082934791431462, Val f1: 0.6476203799247742\n",
      "Val loss: 5.062627616792434, Val f1: 0.6466737985610962\n",
      "\n",
      "starting Epoch 42\n",
      "Training...\n",
      "Train loss: 0.038019582670389476\n",
      "Train loss: 0.03843540090996201\n",
      "Train loss: 0.04019365195350559\n",
      "Train loss: 0.04168424744623679\n",
      "Train loss: 0.04354624989609365\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03581884859924074, Val f1: 0.9850477576255798\n",
      "Val loss: 0.03533638856233707, Val f1: 0.9841868877410889\n",
      "Val loss: 0.0354979490604475, Val f1: 0.9843403100967407\n",
      "Val loss: 0.03494546264390447, Val f1: 0.9844317436218262\n",
      "Val loss: 0.03465031661577212, Val f1: 0.9842170476913452\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 4.874274292698613, Val f1: 0.6487196087837219\n",
      "Val loss: 4.9630926332790475, Val f1: 0.6440693736076355\n",
      "Val loss: 5.0070144969063834, Val f1: 0.6431629657745361\n",
      "Val loss: 5.007039296034291, Val f1: 0.6448670029640198\n",
      "Val loss: 4.973401603186078, Val f1: 0.6438299417495728\n",
      "\n",
      "starting Epoch 43\n",
      "Training...\n",
      "Train loss: 0.03509959759994202\n",
      "Train loss: 0.037594154500992634\n",
      "Train loss: 0.039320459967086246\n",
      "Train loss: 0.04191749677144239\n",
      "Train loss: 0.042778978088840874\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03136846428846372, Val f1: 0.9869397282600403\n",
      "Val loss: 0.032877292675808426, Val f1: 0.9857526421546936\n",
      "Val loss: 0.034286897845650796, Val f1: 0.985478937625885\n",
      "Val loss: 0.03404081081587318, Val f1: 0.985205888748169\n",
      "Val loss: 0.034254350761531056, Val f1: 0.9848130941390991\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.187764428280018, Val f1: 0.6551506519317627\n",
      "Val loss: 5.2457215187734345, Val f1: 0.651215136051178\n",
      "Val loss: 5.263161844937748, Val f1: 0.649224579334259\n",
      "Val loss: 5.219870696849366, Val f1: 0.6496288180351257\n",
      "Val loss: 5.194114070051022, Val f1: 0.6489730477333069\n",
      "\n",
      "starting Epoch 44\n",
      "Training...\n",
      "Train loss: 0.037503671479377915\n",
      "Train loss: 0.039927589463355444\n",
      "Train loss: 0.040412052772213825\n",
      "Train loss: 0.041441843915652\n",
      "Train loss: 0.04284621476590467\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.031586874538656266, Val f1: 0.9862082004547119\n",
      "Val loss: 0.0320360143581504, Val f1: 0.9854369759559631\n",
      "Val loss: 0.03312240792929649, Val f1: 0.9846513867378235\n",
      "Val loss: 0.03346822229206505, Val f1: 0.9844721555709839\n",
      "Val loss: 0.03326107944253257, Val f1: 0.9844996929168701\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.311942522614091, Val f1: 0.6572468280792236\n",
      "Val loss: 5.378426552698621, Val f1: 0.6525937914848328\n",
      "Val loss: 5.44340076947388, Val f1: 0.650127649307251\n",
      "Val loss: 5.456311634781172, Val f1: 0.650302529335022\n",
      "Val loss: 5.433262588085938, Val f1: 0.6494404673576355\n",
      "\n",
      "starting Epoch 45\n",
      "Training...\n",
      "Train loss: 0.0356314473499684\n",
      "Train loss: 0.036793424748662165\n",
      "Train loss: 0.038724659179652385\n",
      "Train loss: 0.040621913265970866\n",
      "Train loss: 0.04126450538767754\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.02970651841669376, Val f1: 0.9874153733253479\n",
      "Val loss: 0.03110625248608632, Val f1: 0.9861201047897339\n",
      "Val loss: 0.031800464919636486, Val f1: 0.9856135249137878\n",
      "Val loss: 0.03202437220904694, Val f1: 0.9854745864868164\n",
      "Val loss: 0.032039387179528526, Val f1: 0.985411524772644\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.345362997938086, Val f1: 0.652215838432312\n",
      "Val loss: 5.42076794467729, Val f1: 0.6522648334503174\n",
      "Val loss: 5.471119935155207, Val f1: 0.6498914361000061\n",
      "Val loss: 5.452943832615243, Val f1: 0.6495275497436523\n",
      "Val loss: 5.436380216931384, Val f1: 0.6484098434448242\n",
      "\n",
      "starting Epoch 46\n",
      "Training...\n",
      "Train loss: 0.032029105370780075\n",
      "Train loss: 0.03508056627660518\n",
      "Train loss: 0.03786283209615305\n",
      "Train loss: 0.039423206981791865\n",
      "Train loss: 0.040921450568192005\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03273625824739212, Val f1: 0.9863264560699463\n",
      "Val loss: 0.03325447100064447, Val f1: 0.9858604669570923\n",
      "Val loss: 0.032868008506709935, Val f1: 0.9853398203849792\n",
      "Val loss: 0.0335755867870412, Val f1: 0.9849361181259155\n",
      "Val loss: 0.03359309284381157, Val f1: 0.9847320914268494\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.273741691200821, Val f1: 0.6458358764648438\n",
      "Val loss: 5.27038185666848, Val f1: 0.644112765789032\n",
      "Val loss: 5.279483483962403, Val f1: 0.6431571841239929\n",
      "Val loss: 5.229724507854349, Val f1: 0.6436871886253357\n",
      "Val loss: 5.2165175427919985, Val f1: 0.6434975266456604\n",
      "\n",
      "starting Epoch 47\n",
      "Training...\n",
      "Train loss: 0.03191308804982186\n",
      "Train loss: 0.03670190510575816\n",
      "Train loss: 0.037830481111993816\n",
      "Train loss: 0.03897162908077289\n",
      "Train loss: 0.04039988679117306\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03490856775301517, Val f1: 0.9855180978775024\n",
      "Val loss: 0.03441751503928383, Val f1: 0.9850865006446838\n",
      "Val loss: 0.03403425631800812, Val f1: 0.9848892688751221\n",
      "Val loss: 0.03392158237555622, Val f1: 0.9847730398178101\n",
      "Val loss: 0.03481551457257052, Val f1: 0.9845289587974548\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.935705632633633, Val f1: 0.6681653261184692\n",
      "Val loss: 6.0226888419077405, Val f1: 0.6632657051086426\n",
      "Val loss: 6.013360306437537, Val f1: 0.661819338798523\n",
      "Val loss: 5.994876052793218, Val f1: 0.6616002321243286\n",
      "Val loss: 5.985057821329985, Val f1: 0.6612346768379211\n",
      "\n",
      "starting Epoch 48\n",
      "Training...\n",
      "Train loss: 0.03365134544247663\n",
      "Train loss: 0.037080813444773174\n",
      "Train loss: 0.038370723869165194\n",
      "Train loss: 0.03954339271215053\n",
      "Train loss: 0.04069389871993784\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.03276130012531333, Val f1: 0.9862170815467834\n",
      "Val loss: 0.03318833233604866, Val f1: 0.9853406548500061\n",
      "Val loss: 0.033694650518186424, Val f1: 0.9844203591346741\n",
      "Val loss: 0.03372050456895403, Val f1: 0.9845128655433655\n",
      "Val loss: 0.0336813892289755, Val f1: 0.9844738245010376\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 6.115229363353164, Val f1: 0.6737291812896729\n",
      "Val loss: 6.2297471196449115, Val f1: 0.6695590019226074\n",
      "Val loss: 6.244542575146115, Val f1: 0.6680676341056824\n",
      "Val loss: 6.232989754575831, Val f1: 0.6676600575447083\n",
      "Val loss: 6.191726691505168, Val f1: 0.6657222509384155\n",
      "\n",
      "starting Epoch 49\n",
      "Training...\n",
      "Train loss: 0.032831310397881765\n",
      "Train loss: 0.034696406925935085\n",
      "Train loss: 0.036524643456206596\n",
      "Train loss: 0.03736776404913742\n",
      "Train loss: 0.039080448346681985\n",
      "\n",
      "Evaluating on train...\n",
      "Val loss: 0.035554051821504985, Val f1: 0.9843354821205139\n",
      "Val loss: 0.03655783000733695, Val f1: 0.983285129070282\n",
      "Val loss: 0.03663426528152631, Val f1: 0.9830495119094849\n",
      "Val loss: 0.03708635670981678, Val f1: 0.9826509356498718\n",
      "Val loss: 0.037399294460779976, Val f1: 0.9826651215553284\n",
      "\n",
      "Evaluating on test...\n",
      "Val loss: 5.2375710681632714, Val f1: 0.6328243613243103\n",
      "Val loss: 5.296802671193197, Val f1: 0.6311591863632202\n",
      "Val loss: 5.262618056093445, Val f1: 0.6288346648216248\n",
      "Val loss: 5.252104391270141, Val f1: 0.6286481618881226\n",
      "Val loss: 5.242083948560948, Val f1: 0.6266806721687317\n",
      "Wall time: 31min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = []\n",
    "losses_eval = []\n",
    "f1s = []\n",
    "f1s_eval = []\n",
    "\n",
    "for i in range(50):\n",
    "    print(f'\\nstarting Epoch {i}')\n",
    "    print('Training...')\n",
    "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
    "    losses.append(epoch_loss)\n",
    "    print('\\nEvaluating on train...')\n",
    "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
    "    f1s.append(f1_on_train)\n",
    "    print('\\nEvaluating on test...')\n",
    "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
    "    losses_eval.append(epoch_loss_on_test)\n",
    "    f1s_eval.append(f1_on_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "eH7ws8dXmEn5",
    "outputId": "3e742ebc-e57f-4d67-eb70-36240964a9d0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.plot(losses_eval)\n",
    "plt.title('BCE loss value')\n",
    "plt.ylabel('BCE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "q32B6i6BmGOc",
    "outputId": "9c885f61-e361-4622-d564-001b386fbcbb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i.cpu() for i in f1s])\n",
    "plt.plot([i.cpu() for i in f1s_eval])\n",
    "plt.title('f1 value')\n",
    "plt.ylabel('f1 value')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достаточно посмотреть на эти графики, чтобы понять очевидное -- мы переобучились! И нам придётся что-нибудь с этим делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 c дропаутом и уменьшением lr и количества эпох\n",
      "0.6257590651512146\n",
      "loss c дропаутом и уменьшением lr и количества эпох\n",
      "5.23437500157777\n"
     ]
    }
   ],
   "source": [
    "print('f1 c дропаутом и уменьшением lr и количества эпох')\n",
    "print(f1s_eval[-1].item())\n",
    "print('loss c дропаутом и уменьшением lr и количества эпох')\n",
    "print(losses_eval[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj0-XaCkYz8w"
   },
   "source": [
    "Для анализа ошибок можно посмотреть на те примеры, которые мы (не)правильно предсказываем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "bHtAIt5eCrP5"
   },
   "outputs": [],
   "source": [
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "    fp = []\n",
    "    fn = []\n",
    "    tp = [] \n",
    "    tn = []\n",
    "    with torch.no_grad():\n",
    "        for i, (texts, ys) in enumerate(iterator):   \n",
    "            preds = model(texts)  # делаем предсказания на тесте \n",
    "            for pred, gold, text in zip(preds, ys, texts):\n",
    "                text = ''.join([id2word[int(word)] for word in text if word !=0])\n",
    "                if round(pred.item()) > gold:\n",
    "                    fp.append(text)\n",
    "                elif round(pred.item()) < gold:\n",
    "                    fn.append(text)\n",
    "                elif round(pred.item()) == gold == 1:\n",
    "                    tp.append(text)\n",
    "                elif round(pred.item()) == gold == 0:\n",
    "                    tn.append(text)\n",
    "    return fp, fn, tp, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "phc1BAE4MKhU"
   },
   "outputs": [],
   "source": [
    "fp, fn, tp, tn = predict(model, val_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(fp, fn, tp, tn):\n",
    "    all_preds = len(fp) + len(fn) + len(tp) + len(tn)\n",
    "    print('accuracy')\n",
    "    print((len(tp) + len(tn))/all_preds)\n",
    "    print('precision')\n",
    "    print(len(tp)/(len(fp) + len(tp)))\n",
    "    print('recall')\n",
    "    print(len(tp)/(len(fn) + len(tp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.641832229580574\n",
      "precision\n",
      "0.6634620199288969\n",
      "recall\n",
      "0.5994118977606876\n"
     ]
    }
   ],
   "source": [
    "get_metrics(fp, fn, tp, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFiLocEC6dUa",
    "outputId": "1f9c6572-ae90-4cbb-e767-b1485882fa2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "что правильно предсказываем: ['даонионидумаличтомыиправдатакиенасамомтоделевсевсенормально', 'rtнезадаютэтобесплатныйсайтличноотебечтомы', 'rtедуввокругтакая', 'rtпозвонилавыздоравливай', 'недостаточноатемаобъяснилэточтоуменядрузьянепо', 'воченьнебольшойив', 'rtтакидедуанапочтеотдельныйящикраньшетак', 'поздравляювассднемделаюисполнения', 'rtспокойнойванв', 'ятолькохотелутебякактыпопалввсвободныйот', 'ябезумноусталазасегодняшниймоимроднымзаэтотвечер!', 'такбыисъел', 'ещенукакзато', 'какжеячтонекоторыелюдибылинамоемсулыбкой', 'сижуввдетскомтеатре—братдевушкииграетсразу2', 'дачтобктебепришел', 'rtтымнеяведьведусебякактебенравятсятольковтвоём', 'акаквышлидумалисбудетскучноинектонемыглубоко', 'когдабабушказаходитвавтобусвсявнезапно', 'собрал', 'какзафотовинстаграмспасибо', 'ивамтоженастроенияещё', 'завтраквпостель2ссыромичашкачерногочаялучшее', 'кпредыдущемууменяестьдрузьястакчтонаихужепримеремогу', 'rtсднембудешь', 'выкакаявыключаювнавсякиймалоли', 'янекогдатыговоришь//теперь', 'наувиделиуменяиотцаномера', 'тебяноимейутебяплохиеоценкипоалгебреивсемтакговорит', 'яипростоотнастолько', 'которыепоцелуйвызываетвсэмоциямивовремяс', 'случайнослучайвоспользоваться', 'яужеднятрисайтпотриразанаиура!', 'уменялюблюкомунравятсямои', 'емуиемунужентеплои', 'rtхаханучто', 'низатипо', 'чтопридумалавиолеттачтобыуспелнаспектакль', 'ойачтооооябывообщееевсю', 'умеетподнятьсюрпризпосле', 'лежатьнаполуэтокемне', 'rtреклама80', 'смотритолькотамгдеане', 'такидолжноведьнедаромон', 'вотчемязанимаюсьна', 'ойкакздоровоуменявсенормально', 'нусегодняивможнобылозагадать', 'rtячувствуюэтодолгожданноелетобудетдлянасоченьсчастливыми', 'всёюляужеспокойнойночи', 'нелинашемнебыловесело:d', 'всегдатакиес', 'rtспасибокоторыесвоиобещаниянисмотряниначтои', 'контрольнаябылапоисториикраявучебникепочтивсескнижкисписала', 'rtвсеаккумулятороставлюнавсякийнадеюсьнаконференциибудетирозеткануилипот', 'аеслиьизприэтомтвоидатыбудут3014тобилетыв28тысрублейилететьс1', 'яделаюэтомой', 'нуунасобщиезнакомыестобойот', 'мненравится', 'люблю/вселенная/', 'пальцынамоялюбимая', 'тутзастоломсидитединственнаямои', 'из-занеесейчаспойдуслушатьпесниизэтого', 'внезапночутьсердцене', 'мнечтотогеометрия', 'таки', 'планзанятийнауспокоитьпоговоритьсуспокоить', 'именнозаэтотгодмояжизньвлучшуюсторону', 'шлюха', 'отличныйсладоститеперьвдваразабольше', 'будемпользоватьсяимыже', 'унасмегаупртрокпонивстиленовогоальбома+в', 'rtкупальникслегканаулице', 'rtмолодцы', 'хорошийхорошийвсечтоменеепроисходиломне', 'улучшихдрузейнетхорошихфотографийтолькояпростопромолчу', 'желаюкаждойизнас', 'отличнопоигралаполовинуигрызавтраигру', 'rtснеговикна', 'rtяв5днейлежалаза18', 'неетысспокойнопойдемполучать', 'боюсьмыскоропоскайпусдрузьямибудемпраздники', 'rt', 'шелденьреволюцииивродеукраинцыненорекордбьют', 'нехренжалетьрукиипоставил', 'брокудаещетыитакумный', 'rtпроземлю', 'датыкакновогодняяёлка', 'rtтылюбишьнутакдавайятебевотстас', 'сказатьчтоялюблюквсе', 'rtизаменитнамgooglemusic', 'чтобтебятакжеитебенезахочетсязаниматьсяабсолютнони', 'унасобычнолюбойразговорраноилипоздноктакчтообещатьненопостараюсь', 'куваснаулицувыходит', 'rtтаквыи', 'такпитьинадо:dможетничегострашноготыине', 'ожезаменякто-тоспасибобудутыне', 'якактыночьюиздомарассказывай', 'привитпростопрелесть', 'янепередвторымэкзаменомчтомогу', 'игрушкусобираюизизчерезинтересно']\n"
     ]
    }
   ],
   "source": [
    "print('что правильно предсказываем:', tp[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeBGHMjm4LBJ",
    "outputId": "455b8bda-35c1-4d09-b4f7-117091e7a1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ошибочно не относим к фамилиям: ['меняужебеситтвоетупоесвоипринципыпрекратисратьвдушуатоубиратьзатобойбудеттолькотвоя', 'атокаквсечтонужноивточтоне', 'имя17минвкуснаякурица', 'какойсамыйкрутойзачет', 'нельзяживотнымахотелось', 'проспала15чувствуюсебясамымчеловекомна', '', 'ярешилазабитьнамодныесейчассериалыиначаласмотретьссамоготакподнимают', 'почемумолодымлюдямтакненравитсяженскаяобувь', 'rtнуещенеанастроениеуже', 'rtпотокскомпасмузыкойнааоттудачерезнаушникио_о', 'чтоэтозафотографии', 'есликвамсночевкоймногоивамнегдеихвозьмитепобольшеионисаминайдутсебе', 'насамомделеявшокес', 'менясилавзрывэмоций', 'чтояменявсегоипоканечтосигаретуменяпздц', 'утебяочэтоточно!', 'ядавнохотелимнонеполучитсялиачегонео_о', 'телефонненадо?а—ахахахах', 'оооообылоавбылилишь', 'ударвчелюстьзаменяет3часаэтопровмнетаклегче', 'забытьвыбывасхотябы', 'даточно', 'нущитонепросто', 'rtкрасиваялюблюпольшу', 'пиздецменяаааподпеснилюбимойнезряжзачеткаколятут', 'нувсезимниеканикулымнеспасибо', 'смотрюнаэтихдураковиузнаюнадобытолькоможнобыловремявернутьназад', 'ойнеправильнояимелаввидуянастолькотимчтозабылаонихнакакоето', 'естьужекоекакиепланынангнадеюсьменяотпустятбудетжарко', 'rtиногдаменя', 'rtбудущегоотдействительнолетаютнад', 'прекраснымутроделаетсондо', 'проститеконечнонопочемуявсевремячитаю', 'илюбимыемоина', 'rtнелюблюноэтокартина', 'rtуженочьбезмыс', 'rtвпятницупойдёмбухатьпосле', 'недумалачтоновсёза', 'верните', 'какисамаяэтоскороахахах', 'ятебятыменянеаятебебатареюпосадила', 'люблюдолгосмотретьдевушкамвунихтакаяновтожевремямилая', 'немногонехотьинользадобрейшегодруги', 'моясестра', 'отличнаяещёвктогдавообще', 'приготовилавпервыйсказали', 'внехочетидти', 'винолучшечембьеттольковженщиныкуда', 'досталаизродиныпоменямучаетмнетребуетсянаучиться', 'одниговорятнравитсяодниговорятвотвчемответнебудукидатьответыизасквтвиттер', 'иуменямыхоть', 'чтовбудущемдлямоейпрофессиидажестудиюнужнопростотолькоемусказатьоб', 'rtубралии', 'такаяпечальнарыжийнелюбитколу', 'неймар3барсаговно', 'акакпоябытожевтакойситуациисебятак', 'иникакихвопросовкмынемыучимся', 'ябудусегодняявномой', 'аминезаявапщи', 'от', 'чеготыотэтой—любви', 'дабылянатвоемтолькобез', 'длявсухойпогодынужно', 'злаяясказал', 'rtесливодкаидетскорождиее', 'кубезаэтим31долларов', 'даиспокачтоневсёиногданаверноеможно', 'этовсеголишь', 'воталесасгрибамиуже', 'сорокминутназадунасмальчикинеуспелродитьсяаужесомнойялюблютебя', 'rtпоедувидажетакоеяинедумалvia', 'неважновин8немогутюзать2010поэтомуоченьавотбета', 'ничеготакогоненосегодняуменястало2000когдаябылвдаженауроке', 'нукактакисоизоконобычно', 'вгородезжупоследнеевремякаквпорядкедажененапрягает', 'яушелсопотомучточутьнебудужаритьчем', 'ауменяподокнаминакаждоесторонытрассы', 'низачтонеэто', 'двакраямдон', 'какстайлснепонятноочемтвиткомутравку', 'утебяужеестьофициальноеприложениедля—скоро', 'rtяотсвоихдлинных', 'чётакаяная', 'респектpsнелюблю', 'самаисветсломалимненаещевпервыеминуты', 'тыможешьнравится—ноне', 'чтомыделаемвнашемвозрастевсехуйнянавсе', 'нужена', 'помнюкакпрощалисьнавокзалеитефоткибилетаинетолько', 'берегите', 'rtладноя', 'унасвсегораздогрустнеевсеипотомспятвшучуконечно', 'теперьпочемутакувзялпогонять', 'датамещефоткикидаютвидеодруг', 'этонесамоехудшеевтвоем', 'мнетц', 'красоткатаклето', 'иотпускнаонживётвнормальнойстране', 'волосымогуебашитьразныеприческисебе']\n"
     ]
    }
   ],
   "source": [
    "print('ошибочно не относим к фамилиям:', fn[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztrjLRu34e-B",
    "outputId": "8f86977e-ff06-4552-a023-ab55c98974b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ошибочно считаем фамилиями: ['rtвжопе', 'rtзабылаотметитьмойденьрождения29октября', 'янесигратормозитжутко—компне', 'rtnew–дрочунанее', 'ясебеоченьплохотакчтодавайлучшезавтрапойдемвсегулятьиятебетвой', 'теперьуменяестькрутаячтонемного', 'rtпочемутакойсерый—нравится', 'rtуменянапараллелитакаядевка', 'уменятакбарбиневернули', 'атакчтобывсёхорошо', 'яуедунакогдавырасту', 'rtвеликолепнаясериятолько', 'ятожеургантпро', 'почтине', 'ужемоскваспатьпошла', 'деньсвятоговсечтовсе', 'овоттебяитоприятнеемб', 'мнекупилиподарятеготолько29декабряждать15дней', 'пропустилнаминутунетеперьоколостоятьна', 'нунаучебехотяитыпосленгвсе-такиеще', 'надушемнепоможетивановнаиведроснимиибудет', 'япокастоювочередивбанкевсегдапомогаюотправитьпарунаниодинсотрудникбанкани', 'подарилинедавномненаодномдрнувеселаябылабез', 'сегодняпосмотрелднёмфильмнооченьинтереснаяреальная5из', 'чтокак/где/ненас', 'явообщепитьнадобыломеньше', 'да!какздоровоприйтиви38', 'напередчетыревсестоятвключая', 'толькочтоузналчтосегоднячто-то', 'чтоделаютнормпишутинтересныетвитыиретвитятиделаюразговариваютутссобойивсем', 'самаячастьэтона', 'ауменякартинканекрасавца', 'rtатыменявообщене', 'фоткидевушкиятутчтоуменякакуя', 'небудетничего', 'вотненадомнечтовывсеиз', 'rtу', 'блинменяребятавышлив', 'аявотсрадостьюбыпосмотреланаолимпийскийогоньеслибынемоя', 'какуменядела?всевсеоченьоченьплохо', 'каждыйкогдаодноговмиреплачут10', 'янажалнаввелсвоиатеперьтутпростовоттакаякартинкаиникаких', 'кпримеру1янашлаавтороев', 'тожехотяяздороваяимного', 'прямпро', 'однопредметынесчитаяэтуинформатику', 'думаешькрасивыйстатусразитыегоизряявесьинтернет', 'опятьвуфнучтоподвечертоопять', 'этоименнотавещькотораянаменя', 'з', 'rt', 'удевочкивстатусеэтикомментарийтыпросвоюгрудь', 'знаетечтояоойзавтрапятницаойчто', 'сегоднякомневокноимыснейапотомяпроснулась', 'насамомделеоченьтеперьвсетольковотчтобывыучить', 'так-тоисегоднябезшапки', 'выучилабилетпофизикеиуже', 'всебылооченькрутоимногоприятныхзнакомств', 'скорововсеххолодильникстраныоливье', 'завтращаснадоехатьнатакне', 'даженеоченьноиз-заночисолнцаазавтраужеуезжаемкактвои', 'ятожеоченьсильно', 'урокунеевообще', 'приехалсо', 'янепочемуthefoxтакаяодинразможноноэтожеане', 'rt', 'хочусейчасуснувкреслестоматолога', 'сестранекогдаяпришлаутромихотелаподелитьсяснейтакиделай', 'кдвумгодамонивовсене', 'уменяестьмойона', 'делаюпокачтоничегонея', 'ночембольшеячтонанегокучатеммнеинтереснее', 'к', 'явасумоляюнеосебеувасхватаетхотитестанунатольконеговоритеосебеплохо', 'мояспалиламенянасчетнашихтакихяещенеслышалаахах', 'сейчасбыужесиделасосвоимина', 'бывсёнобырядом', 'яопятьсегоднянедочитаютвиттерскийффвсемяушёло/', 'rtтокогдаодинокимдевушкамдарятцветычем', 'высженей—да', 'синтервьюаменяясно', 'rtясно', 'датыявотпипецзомби', 'вмаршруткуисказалноответанебылоличноскаждым', 'селанасамоминтересномместевдоль', 'ненеправильнонезанимаюсьрегулярно', 'автвничегоинтересноговы', 'блянупочемукоторогоялюблюнет', 'вмиреестьадекватныеиестьмоя', 'синтернетомнателефонепроисходиткакая-токоторыемыоказалисьобщем', 'чтоэтоделаетнекомуиначеотобидысошлабыс', 'здесьдолжнабытькартинкойсяинетоноуменяеенет', 'rtахахянислушал', 'япримерностольконанихинеконецэпохи', 'ясдорогиемои', 'rtэтохорошо', 'хотяяскореевесьчм', 'rtнуиголосуменяпросто', 'rtвсеравнолавкиеёпоследнююночьпочтиневсамомконце', 'нузовутвауменяпопас']\n"
     ]
    }
   ],
   "source": [
    "print('ошибочно считаем фамилиями:', fp[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZL6OlYcbE9d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "9_CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
